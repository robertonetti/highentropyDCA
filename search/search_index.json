{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the <code>adabmDCA 2.0</code> Documentation","text":"<p><code>adabmDCA</code> is a versatile library for Direct Coupling Analysis (DCA), enabling the training, sampling, and application of Boltzmann Machines (Potts models) on biological sequence data.</p> <p>Instructons</p> <p>This documentation is meant for providing a user-friendly description of the <code>adabmDCA</code> package main features. It is supported by:</p> <ul> <li>The main article [Rosset et al., 2025], with detailed explanations of the main features. The present documentation is a shorter version of the paper, but it includes additional features</li> <li>The Colab notebook providing a tutorial of the APIs for training, sampling and analyzing a <code>bmDCA</code> model (Python only)</li> </ul> <p>This tutorial introduces the new and enhanced version of <code>adabmDCA</code> [Muntoni at al., 2021]. The software is available in three language-specific implementations:</p> <ul> <li>C++ \u2013 optimized for single-core CPUs  </li> <li>Julia \u2013 ideal for multi-core CPU setups  </li> <li>Python \u2013 GPU-accelerated and feature-rich</li> </ul> <p>All versions share a unified terminal-based interface, allowing users to choose based on their hardware and performance needs.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":""},{"location":"#model-training","title":"\ud83e\udde0 Model Training","text":"<p>Choose from three training strategies to fit your model complexity and goals:</p> <ul> <li><code>bmDCA</code>: Fully-connected Boltzmann Machine [Figliuzzi et al., 2018]</li> <li><code>eaDCA</code>: Sparse model with progressively added couplings [Calvanese et al., 2024]</li> <li><code>edDCA</code>: Prunes an existing <code>bmDCA</code> model down to a sparse network [Barrat-Charlaix et al., 2021]</li> </ul>"},{"location":"#applications-of-pretrained-models","title":"\u2699\ufe0f Applications of Pretrained Models","text":"<p>Once trained, models can be used to:</p> <ul> <li>Generate new sequences</li> <li>Predict structural contacts [Ekeberg et al., 2013].</li> <li>Score sequence datasets based on model energy</li> <li>Build mutational libraries with DCA-based scoring</li> </ul>"},{"location":"#advanced-features-in-python-adabmdcapy","title":"\ud83d\ude80 Advanced Features in Python (<code>adabmDCApy</code>)","text":"<p>The Python version includes exclusive features:</p> <ul> <li>Experimental feedback reintegration for refined models [Calvanese et al., 2025]</li> <li>Thermodynamic integration to estimate model entropy</li> <li><code>Profmark</code>: GPU-accelerated dataset splitting with phylogenetic and sampling bias control, based on the <code>cobalt</code> algorithm [Petti et al., 2022]</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run? Skip ahead to the Quicklist for command-line usage examples.</p>"},{"location":"applications/","title":"Applications","text":"<p>Info</p> <p>We report in the Script arguments section the list of all the possible input arguments of each routine. The same information can be shown from the command line using:</p> <p><code>adabmDCA &lt;routine_name&gt; -h</code></p>"},{"location":"applications/#generate-sequences","title":"\ud83e\uddec Generate Sequences","text":"<p>Once a model is trained, it can be used to generate new sequences with:</p> <pre><code>adabmDCA sample -p &lt;path_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li><code>&lt;output_folder&gt;</code>: directory to save the output.</li> <li><code>&lt;num_gen&gt;</code>: number of sequences to generate.</li> </ul> <p>The tool first estimates the mixing time <code>t_mix</code> by simulating chains from the MSA. It then initializes <code>num_gen</code> Markov chains and runs <code>nmix * t_mix</code> sweeps (default <code>nmix = 2</code>) to ensure thermalization. </p> <p>\ud83d\udce6 Output Files:</p> <ul> <li>A FASTA file of generated sequences</li> <li>A log file for reproducing the mixing time surves (Fig. 3-left)</li> <li>A log file tracking the Pearson \\(C_{ij}\\) score as a function of the sampling time </li> </ul>"},{"location":"applications/#convergence-criterion","title":"Convergence Criterion","text":"<p>To ensure proper sampling, sequence identity is used to track mixing:</p> <ul> <li>\\(\\mathrm{SeqID}(t)\\) = identity between pairs of independent samples</li> <li>\\(\\mathrm{SeqID}(t, t/2)\\) = identity between the same chain at different times</li> </ul> <p>Denoting \\(\\pmb{a}_i(t)\\) the i-th sequence of the MSA at sampling time \\(t\\), these are computed as:</p> \\[     \\mathrm{SeqID}(t) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_{\\sigma(i)}(t)) \\qquad \\mathrm{SeqID}(t, t/2) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_i(t/2)) \\] <p>where \\(\\sigma(i)\\) is a random permutation of the index \\(i\\) and</p> \\[     \\mathrm{SeqID}(\\pmb{a}, \\pmb{b}) = \\frac{1}{L}\\sum_{i=1}^L \\delta_{a_i, b_i} \\in [0, 1] \\] <p>Convergence is assumed when \\(\\mathrm{SeqID}(t) \\cong \\mathrm{SeqID}(t, t/2)\\).</p> <p> Figure 3: Analysis of a bmDCA model. Left: measuring the mixing time of the model using \\(10^4\\) chains. The curves represent the average overlap among randomly initialized samples (dark blue) and the one among the same sequences between times \\(t\\) and \\(t/2\\) (light blue). Shaded areas represent the error of the mean. When the two curves merge, we can assume that the chains at time \\(t\\) forgot the memory of the chains at time \\(t/2\\). This point gives us an estimate of the model's mixing time, \\(t^{\\mathrm{mix}}\\).  Notice that the times start from 1, so the starting conditions are not shown. Right: Scatter plot of the entries of the Covariance matrix of the data versus that of the generated samples.</p>"},{"location":"applications/#contact-prediction","title":"\ud83d\udd17 Contact Prediction","text":"<p>One of the principal applications of the DCA models has been that of predicting a tertiary structure of a protein or RNA domain. In particular, with each pair of sites \\(i\\) and \\(j\\) in the MSA, <code>adabmDCA 2.0</code> computes a contact score that quantifies how likely the two associated positions in the chains are in contact in the three-dimensional structure. Formally, it corresponds to the average-product corrected (APC) Frobenius norms of the coupling matrices [Ekeberg et al., 2013], i.e.</p> \\[ F_{i,j}^{\\rm APC} = F_{i,j} - \\frac{\\sum_{k} F_{i,k} \\sum_{k} F_{k,j}}{\\sum_{kl} F_{k,l}}, \\quad F_{i,j} = \\sqrt{\\sum_{a,b \\neq '-'} J_{i,j}\\left(a, b \\right)^{2}} \\] <p>To compute contact scores:</p> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>Zero-sum gauge and gap symbols are handled internally.</p> <p>\ud83d\udce6 Output Files:</p> <ul> <li><code>&lt;label&gt;_frobenius.txt</code> with scores for each pair.</li> </ul>"},{"location":"applications/#sequence-scoring","title":"\ud83d\udcc9 Sequence Scoring","text":"<p>To score sequences using the DCA energy with a trained model:</p> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence is annotated with its statistical energy. Lower energies correspond to more likely (or better fitting) sequences under the model.</li> </ul>"},{"location":"applications/#single-mutant-library","title":"\ud83e\uddea Single Mutant Library","text":"<p>To simulate a mutational scan around a wild-type sequence:</p> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence represents a single-point mutant, named by mutation and \\(\\Delta E\\) (change in energy). Example:</li> </ul> <pre><code>&gt;G27A | DCAscore: -0.6\n</code></pre> <p>Negative \\(\\Delta E\\) suggests improved fitness.</p>"},{"location":"applications/#reintegrated-dca-model-from-experiments","title":"\ud83d\udd01 Reintegrated DCA Model from Experiments","text":"<p>As described in [Calvanese et al., 2025], it is possible to train a DCA model informed with experimental feedback in order to improve the model's ability of generating functional sequences:</p> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --lambda_ &lt;lambda_value&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>nat_msa</code>: MSA of natural sequences</li> <li><code>reint_msa</code>: MSA of tested sequences</li> <li><code>adj_vector</code>: a text file containing experimental results for the reintegration dataset. Each line of the file should contain <code>+1</code> or <code>-1</code>, where the i-th line corresponds to:<ul> <li><code>1</code> if the i-th sequence of the <code>reint_msa</code> passes the experimental test;</li> <li><code>-1</code> if the i-th sequence does not pass the experimental test;</li> </ul> </li> <li><code>lambda_</code>: reintegration strength (default: 1)</li> <li><code>alphabet</code>: <code>protein</code> or <code>rna</code>, sequence type</li> </ul> <p>\ud83d\udca1 Tip: It is possible to use continuous values from -1 to 1 for the <code>adj_vector</code>, depending on the performance of the sequence in the experiment. Additionally, the <code>lambda_</code> parameter can be fine-tuned to adjust the reintegration strength. If unsure, a good starting point is to use <code>lambda_</code> = 1 and \u00b11 values for the <code>adj_vector</code>.</p>"},{"location":"applications/#traintest-split-for-homologous-sequences","title":"\ud83e\udde0 Train/Test Split for Homologous Sequences","text":"<p>When dealing with a family of homologous sequences, splitting data into training and test sets has to be done carefully. There are two main reasons for this:</p> <ol> <li>Since homology introduces correlations between sequences, a simple random split would yield a test set that closely reproduces the training set on any statistical test,</li> <li>Because some regions of the sequence space are sampled more than others, the test set might contain densely populated clusters of sequences that would bias any type of assessment.</li> </ol> <p>To overcome these issues, we propose a simplified GPU-accelerated version of the <code>cobalt</code> algorithm introduced in [Petti et al., 2022]. The algorithm proceeds in two steps:</p> <ol> <li>A first train/test split is done, such that no sequence in the test set has more than <code>t1</code> fractional sequences identity with any sequences in the training set;</li> <li>The test set is pruned until any two sequences in it have fractional sequence identity that does not exceeds the value <code>t2</code>.</li> </ol> <p>Typical usage:</p> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre> <p>Required:</p> <ul> <li><code>t1</code>: max train/test identity</li> <li><code>t2</code>: max identity within test set</li> <li><code>n_trials</code>: number of trials to find best split</li> <li><code>output_prefix</code>: generates <code>&lt;output_prefix&gt;.train</code> and <code>&lt;output_prefix&gt;.test</code> files</li> <li><code>input_msa</code>: input MSA in FASTA format</li> </ul> <p>Optional:</p> <ul> <li><code>-t3</code>: max train/train identity</li> <li><code>--maxtrain</code>, <code>--maxtest</code>: size limits for train and test sets</li> <li><code>--alphabet</code>: sequence type (<code>protein</code>, <code>rna</code>, <code>dna</code>)</li> <li><code>--seed</code>: random seed (default 42)</li> <li><code>--device</code>: computation device (default <code>cuda</code>)</li> </ul>"},{"location":"highentDCA_index/","title":"Welcome to highentDCA Documentation","text":"<p>highentDCA is a specialized Python package for training entropy-decimated Direct Coupling Analysis (edDCA) models on biological sequence data. This package extends the powerful <code>adabmDCA</code> framework to enable efficient training of sparse Potts models while tracking their entropy evolution during the decimation process.</p> <p>About this Documentation</p> <p>This documentation provides a comprehensive guide to using highentDCA for training sparse DCA models with entropy tracking. It complements the adabmDCA documentation and focuses specifically on the entropy decimation features.</p>"},{"location":"highentDCA_index/#what-is-highentdca","title":"What is highentDCA?","text":"<p>highentDCA implements the entropy-decimated DCA (edDCA) algorithm, a method that:</p> <ul> <li>Progressively prunes couplings from a fully-connected Boltzmann Machine (bmDCA)</li> <li>Maintains model accuracy while reducing coupling density to target levels</li> <li>Computes entropy at key decimation checkpoints using thermodynamic integration</li> <li>Provides insights into the relationship between model complexity and information content</li> </ul> <p>This approach is particularly valuable for:</p> <ul> <li>Understanding which interactions are essential for capturing sequence statistics</li> <li>Building interpretable sparse models for protein families</li> <li>Studying the thermodynamics of statistical models</li> <li>Reducing computational requirements for downstream applications</li> </ul>"},{"location":"highentDCA_index/#core-features","title":"Core Features","text":""},{"location":"highentDCA_index/#entropy-decimation-eddca","title":"\ud83d\udd2c Entropy Decimation (edDCA)","text":"<p>The main feature of highentDCA is the ability to train edDCA models that:</p> <ul> <li>Start from a converged bmDCA model (or train one automatically)</li> <li>Iteratively remove the least important couplings based on empirical statistics</li> <li>Re-equilibrate after each decimation step to maintain accuracy</li> <li>Track coupling density, Pearson correlation, and model entropy throughout the process</li> </ul>"},{"location":"highentDCA_index/#thermodynamic-integration","title":"\ud83d\udcca Thermodynamic Integration","text":"<p>At pre-defined density checkpoints, highentDCA computes model entropy using thermodynamic integration:</p> <ul> <li>Introduces a bias parameter \u03b8 towards a target sequence</li> <li>Integrates average sequence identity from \u03b8=0 to \u03b8_max</li> <li>Uses careful equilibration to ensure accurate entropy estimates</li> <li>Saves entropy values for downstream analysis</li> </ul>"},{"location":"highentDCA_index/#flexible-checkpointing","title":"\ud83d\udcbe Flexible Checkpointing","text":"<p>Multiple checkpoint strategies for saving model state:</p> <ul> <li>Linear checkpointing: Save at regular intervals</li> <li>Density-based checkpointing: Save at specific coupling densities</li> <li>Automatic saving of model parameters, chains, and statistics</li> <li>Optional integration with Weights &amp; Biases for experiment tracking</li> </ul>"},{"location":"highentDCA_index/#gpu-acceleration","title":"\ud83d\ude80 GPU Acceleration","text":"<p>Built on PyTorch for efficient computation:</p> <ul> <li>GPU-accelerated training and sampling</li> <li>Efficient parallel Markov chain Monte Carlo</li> <li>Automatic device management (CUDA/CPU)</li> <li>Support for mixed precision (float32/float64)</li> </ul>"},{"location":"highentDCA_index/#how-eddca-works","title":"How edDCA Works","text":"<p>The entropy decimation algorithm follows these steps:</p> <ol> <li>Initialization: Start with a converged bmDCA model or train one</li> <li>Decimation: Remove a fraction of couplings with smallest empirical two-point correlations</li> <li>Equilibration: Run MCMC to equilibrate chains on the decimated graph</li> <li>Re-convergence: Perform gradient descent to match data statistics</li> <li>Entropy Computation: At checkpoints, compute entropy via thermodynamic integration</li> <li>Iteration: Repeat steps 2-5 until target density is reached</li> </ol> <p></p>"},{"location":"highentDCA_index/#key-advantages","title":"Key Advantages","text":""},{"location":"highentDCA_index/#sparsity-with-accuracy","title":"Sparsity with Accuracy","text":"<p>edDCA achieves high sparsity while maintaining model accuracy:</p> <ul> <li>Typical models retain only 2-5% of couplings</li> <li>Pearson correlation with data statistics remains &gt;0.95</li> <li>Essential interactions are preserved</li> </ul>"},{"location":"highentDCA_index/#entropy-tracking","title":"Entropy Tracking","text":"<p>Understanding model information content:</p> <ul> <li>Entropy decreases as couplings are removed</li> <li>Rate of decrease reveals importance of interactions</li> <li>Provides thermodynamic insights into model complexity</li> </ul>"},{"location":"highentDCA_index/#computational-efficiency","title":"Computational Efficiency","text":"<p>Sparse models are faster for downstream applications:</p> <ul> <li>Reduced memory footprint</li> <li>Faster sampling and energy computation</li> <li>Easier interpretation and visualization</li> </ul>"},{"location":"highentDCA_index/#use-cases","title":"Use Cases","text":""},{"location":"highentDCA_index/#protein-contact-prediction","title":"Protein Contact Prediction","text":"<p>edDCA models can identify essential residue-residue contacts:</p> <pre><code>highentDCA train \\\n    --data protein_family.fasta \\\n    --model edDCA \\\n    --density 0.05 \\\n    --output contact_prediction\n</code></pre>"},{"location":"highentDCA_index/#mutational-effect-prediction","title":"Mutational Effect Prediction","text":"<p>Sparse models capture key constraints for sequence function:</p> <pre><code>highentDCA train \\\n    --data enzyme_family.fasta \\\n    --model edDCA \\\n    --density 0.03 \\\n    --alphabet protein\n</code></pre>"},{"location":"highentDCA_index/#thermodynamic-analysis","title":"Thermodynamic Analysis","text":"<p>Study entropy evolution during decimation:</p> <pre><code>highentDCA train \\\n    --data sequence_data.fasta \\\n    --model edDCA \\\n    --theta_max 5.0 \\\n    --nsteps 100\n</code></pre>"},{"location":"highentDCA_index/#getting-started","title":"Getting Started","text":"<p>New to highentDCA? Follow these steps:</p> <ol> <li>Installation: Set up the package and dependencies</li> <li>Quick Start: Train your first edDCA model</li> <li>CLI Reference: Explore all available options</li> <li>API Documentation: Use highentDCA in Python scripts</li> <li>Examples: Learn from practical examples</li> </ol>"},{"location":"highentDCA_index/#comparison-with-bmdca","title":"Comparison with bmDCA","text":"Feature bmDCA edDCA (highentDCA) Coupling density 100% (fully connected) 2-10% (sparse) Training time Standard Longer (includes decimation) Memory usage High Lower (sparse parameters) Interpretability Complex Better (fewer interactions) Entropy tracking No Yes Use case General-purpose Sparsity &amp; thermodynamics"},{"location":"highentDCA_index/#integration-with-adabmdca","title":"Integration with adabmDCA","text":"<p>highentDCA is built on top of adabmDCA and shares:</p> <ul> <li>Data formats: Compatible FASTA input and parameter formats</li> <li>Sampling methods: Same Gibbs and Metropolis samplers</li> <li>Statistics functions: Identical frequency and correlation computations</li> <li>Utilities: Common helper functions for encoding, I/O, etc.</li> </ul> <p>You can use adabmDCA models as starting points for edDCA training, and edDCA outputs are compatible with adabmDCA analysis tools.</p>"},{"location":"highentDCA_index/#technical-requirements","title":"Technical Requirements","text":""},{"location":"highentDCA_index/#software-dependencies","title":"Software Dependencies","text":"<ul> <li>Python \u2265 3.10</li> <li>PyTorch \u2265 2.1.0 (with CUDA recommended)</li> <li>adabmDCA == 0.5.0</li> <li>NumPy, Pandas, Matplotlib, BioPython</li> </ul>"},{"location":"highentDCA_index/#hardware-recommendations","title":"Hardware Recommendations","text":"<ul> <li>GPU: NVIDIA GPU with CUDA support (recommended)</li> <li>Minimum 4GB VRAM for small datasets</li> <li>8GB+ VRAM for large protein families</li> <li>CPU: Multi-core processor for data preprocessing</li> <li>RAM: 8GB+ depending on dataset size</li> </ul>"},{"location":"highentDCA_index/#dataset-requirements","title":"Dataset Requirements","text":"<ul> <li>Multiple sequence alignment in FASTA format</li> <li>Minimum ~1000 sequences (more is better)</li> <li>Quality-controlled alignment (gaps, truncations handled)</li> <li>Compatible alphabets: protein, RNA, DNA, or custom</li> </ul>"},{"location":"highentDCA_index/#support-and-community","title":"Support and Community","text":""},{"location":"highentDCA_index/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Read the full documentation in the <code>docs/</code> folder</li> <li>Issues: Report bugs on GitHub Issues</li> <li>Questions: Contact robertonetti3@gmail.com</li> </ul>"},{"location":"highentDCA_index/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Areas for improvement:</p> <ul> <li>Additional checkpoint strategies</li> <li>Alternative decimation algorithms</li> <li>Visualization tools for entropy analysis</li> <li>Extended entropy computation methods</li> <li>Documentation and examples</li> </ul>"},{"location":"highentDCA_index/#related-resources","title":"Related Resources","text":""},{"location":"highentDCA_index/#papers","title":"Papers","text":"<ul> <li>Entropy Decimation: Barrat-Charlaix et al., 2021</li> <li>Adaptive bmDCA: Muntoni et al., 2021</li> <li>DCA for Contact Prediction: Ekeberg et al., 2013</li> </ul>"},{"location":"highentDCA_index/#software","title":"Software","text":"<ul> <li>adabmDCApy: Python implementation</li> <li>adabmDCA.jl: Julia implementation</li> <li>adabmDCAc: C++ implementation</li> </ul>"},{"location":"highentDCA_index/#tutorials","title":"Tutorials","text":"<ul> <li>adabmDCA Colab Notebook</li> <li>DCA Tutorial</li> </ul>"},{"location":"highentDCA_index/#license","title":"License","text":"<p>highentDCA is released under the Apache License 2.0. See LICENSE for details.</p>"},{"location":"highentDCA_index/#citation","title":"Citation","text":"<p>If you use highentDCA in your research, please cite:</p> <pre><code>@software{highentDCA2024,\n  author = {Netti, Roberto and Weigt, Martin},\n  title = {highentDCA: Entropy-decimated Direct Coupling Analysis},\n  year = {2024},\n  url = {https://github.com/robertonetti/highentropyDCA}\n}\n</code></pre> <p>Ready to get started? Head to the Installation Guide \u2192</p>"},{"location":"highentDCA_installation/","title":"Installation Guide","text":"<p>This guide provides detailed instructions for installing highentDCA on your system.</p>"},{"location":"highentDCA_installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing highentDCA, ensure you have the following prerequisites:</p>"},{"location":"highentDCA_installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Linux, macOS, or Windows with WSL2</li> <li>Python: Version 3.10 or higher</li> <li>GPU (recommended): NVIDIA GPU with CUDA support for optimal performance</li> <li>Memory: At least 8GB RAM (16GB+ recommended for large datasets)</li> </ul>"},{"location":"highentDCA_installation/#python-environment","title":"Python Environment","text":"<p>We strongly recommend using a virtual environment to avoid dependency conflicts:</p> <pre><code># Using conda (recommended)\nconda create -n highentdca python=3.10\nconda activate highentdca\n\n# Or using venv\npython -m venv highentdca_env\nsource highentdca_env/bin/activate  # On Windows: highentdca_env\\Scripts\\activate\n</code></pre>"},{"location":"highentDCA_installation/#installation-methods","title":"Installation Methods","text":""},{"location":"highentDCA_installation/#method-1-install-from-source-recommended","title":"Method 1: Install from Source (Recommended)","text":"<p>This is currently the only installation method since the package is not yet on PyPI.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/robertonetti/highentropyDCA.git\ncd highentropyDCA\n</code></pre> <ol> <li>Install the package:</li> </ol> <pre><code>pip install .\n</code></pre> <p>This will automatically install all required dependencies.</p> <ol> <li>Verify installation:</li> </ol> <pre><code>highentDCA --help\n</code></pre> <p>You should see the help message with available commands.</p>"},{"location":"highentDCA_installation/#method-2-development-installation","title":"Method 2: Development Installation","text":"<p>If you plan to modify the code or contribute to the project:</p> <pre><code>git clone https://github.com/robertonetti/highentropyDCA.git\ncd highentropyDCA\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in \"editable\" mode, so changes to the source code are immediately reflected without reinstalling.</p>"},{"location":"highentDCA_installation/#dependencies","title":"Dependencies","text":"<p>highentDCA requires the following packages, which are installed automatically:</p>"},{"location":"highentDCA_installation/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>adabmDCA (== 0.5.0): Base DCA framework with core algorithms</li> <li>PyTorch (&gt;= 2.1.0): Deep learning framework for GPU acceleration</li> <li>NumPy (&gt;= 1.26.4): Numerical computing library</li> <li>Pandas (&gt;= 2.2.2): Data manipulation and analysis</li> </ul>"},{"location":"highentDCA_installation/#additional-dependencies","title":"Additional Dependencies","text":"<ul> <li>Matplotlib (&gt;= 3.8.0): Plotting and visualization</li> <li>tqdm (&gt;= 4.66.6): Progress bars for long-running operations</li> <li>BioPython (&gt;= 1.85): Biological sequence file handling</li> <li>wandb (&gt;= 0.12.0): Experiment tracking (optional, for <code>--wandb</code> flag)</li> </ul>"},{"location":"highentDCA_installation/#gpu-support","title":"GPU Support","text":""},{"location":"highentDCA_installation/#cuda-installation","title":"CUDA Installation","text":"<p>For optimal performance, install PyTorch with CUDA support:</p> <ol> <li>Check CUDA version:</li> </ol> <pre><code>nvidia-smi\n</code></pre> <p>Look for the CUDA version in the output (e.g., CUDA 11.8, 12.1).</p> <ol> <li>Install PyTorch with CUDA:</li> </ol> <p>Visit PyTorch's installation page and select your configuration. For example:</p> <pre><code># For CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# For CUDA 12.1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <ol> <li>Verify GPU support:</li> </ol> <pre><code>python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre> <p>Should output: <code>CUDA available: True</code></p>"},{"location":"highentDCA_installation/#cpu-only-installation","title":"CPU-Only Installation","text":"<p>If you don't have a GPU or prefer CPU-only execution:</p> <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre> <p>Note: Training will be significantly slower on CPU.</p>"},{"location":"highentDCA_installation/#installing-adabmdca","title":"Installing adabmDCA","text":"<p>highentDCA depends on adabmDCA version 0.5.0. This will be installed automatically, but you can also install it manually:</p>"},{"location":"highentDCA_installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install adabmDCA==0.5.0\n</code></pre>"},{"location":"highentDCA_installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/spqb/adabmDCApy.git\ncd adabmDCApy\ngit checkout v0.5.0  # Ensure correct version\npip install .\n</code></pre>"},{"location":"highentDCA_installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"highentDCA_installation/#common-issues","title":"Common Issues","text":""},{"location":"highentDCA_installation/#issue-command-not-found-highentdca","title":"Issue: <code>command not found: highentDCA</code>","text":"<p>Solution: Ensure the installation directory is in your PATH:</p> <pre><code># Check if pip bin directory is in PATH\npip show highentDCA | grep Location\n</code></pre> <p>Add to PATH if needed:</p> <pre><code>export PATH=\"$HOME/.local/bin:$PATH\"  # Add to ~/.bashrc or ~/.zshrc\n</code></pre>"},{"location":"highentDCA_installation/#issue-cuda-out-of-memory-errors","title":"Issue: CUDA out of memory errors","text":"<p>Solutions: - Reduce the number of chains: <code>--nchains 5000</code> (instead of default 10000) - Use smaller batch sizes for sampling - Monitor GPU memory: <code>nvidia-smi -l 1</code></p>"},{"location":"highentDCA_installation/#issue-importerror-for-adabmdca-modules","title":"Issue: ImportError for adabmDCA modules","text":"<p>Solution: Ensure adabmDCA is correctly installed:</p> <pre><code>python -c \"import adabmDCA; print(adabmDCA.__version__)\"\n</code></pre> <p>Should output: <code>0.5.0</code></p> <p>If not, reinstall:</p> <pre><code>pip uninstall adabmDCA\npip install adabmDCA==0.5.0\n</code></pre>"},{"location":"highentDCA_installation/#issue-weights-biases-login-required","title":"Issue: Weights &amp; Biases login required","text":"<p>Solution: Initialize wandb (only needed if using <code>--wandb</code> flag):</p> <pre><code>wandb login\n</code></pre> <p>Enter your API key from wandb.ai.</p>"},{"location":"highentDCA_installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"highentDCA_installation/#macos","title":"macOS","text":"<p>On macOS with Apple Silicon (M1/M2/M3):</p> <pre><code># Use conda for better compatibility\nconda create -n highentdca python=3.10\nconda activate highentdca\nconda install pytorch::pytorch -c pytorch\npip install adabmDCA==0.5.0\npip install .\n</code></pre> <p>Note: MPS (Metal Performance Shaders) acceleration is supported but may not be as optimized as CUDA.</p>"},{"location":"highentDCA_installation/#windows","title":"Windows","text":"<p>On Windows, use WSL2 (Windows Subsystem for Linux) for the best experience:</p> <ol> <li>Install WSL2 with Ubuntu</li> <li>Follow the Linux installation instructions inside WSL2</li> <li>For GPU support, install CUDA on WSL2</li> </ol>"},{"location":"highentDCA_installation/#linux-docker","title":"Linux (Docker)","text":"<p>For a containerized environment:</p> <pre><code>FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.10 \\\n    python3-pip \\\n    git\n\nRUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\nRUN git clone https://github.com/robertonetti/highentropyDCA.git &amp;&amp; \\\n    cd highentropyDCA &amp;&amp; \\\n    pip3 install .\n\nCMD [\"bash\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t highentdca .\ndocker run --gpus all -it highentdca\n</code></pre>"},{"location":"highentDCA_installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify everything works:</p>"},{"location":"highentDCA_installation/#1-check-cli-access","title":"1. Check CLI Access","text":"<pre><code>highentDCA train --help\n</code></pre> <p>Should display the training command help.</p>"},{"location":"highentDCA_installation/#2-test-python-import","title":"2. Test Python Import","text":"<pre><code>import highentDCA\nfrom highentDCA.models.edDCA import fit\nfrom highentDCA.checkpoint import Checkpoint\nprint(\"highentDCA successfully imported!\")\n</code></pre>"},{"location":"highentDCA_installation/#3-run-a-quick-test","title":"3. Run a Quick Test","text":"<pre><code># Download test data\ncd highentropyDCA/example_data\n\n# Train a small model\nhighentDCA train \\\n    --data TEST/chains.fasta \\\n    --output test_output \\\n    --model edDCA \\\n    --density 0.98 \\\n    --drate 0.01 \\\n    --nchains 1000 \\\n    --nepochs 100 \\\n    --seed 42\n</code></pre> <p>This should complete without errors and create output in <code>test_output/</code>.</p>"},{"location":"highentDCA_installation/#updating-highentdca","title":"Updating highentDCA","text":"<p>To update to the latest version:</p> <pre><code>cd highentropyDCA\ngit pull origin main  # or develop\npip install --upgrade .\n</code></pre> <p>For development installations (<code>pip install -e .</code>), just pull the latest code:</p> <pre><code>git pull origin main\n</code></pre>"},{"location":"highentDCA_installation/#uninstalling","title":"Uninstalling","text":"<p>To remove highentDCA:</p> <pre><code>pip uninstall highentDCA\n</code></pre> <p>To also remove dependencies:</p> <pre><code>pip uninstall highentDCA adabmDCA torch numpy pandas matplotlib tqdm biopython wandb\n</code></pre>"},{"location":"highentDCA_installation/#next-steps","title":"Next Steps","text":"<p>Now that you have highentDCA installed:</p> <ol> <li>Quick Start: Train your first model</li> <li>Usage Guide: Explore all features and options</li> <li>API Reference: Use highentDCA in Python scripts</li> <li>Examples: Learn from practical examples</li> </ol>"},{"location":"highentDCA_installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ul> <li>Check the GitHub Issues</li> <li>Review the adabmDCA documentation</li> <li>Contact: robertonetti3@gmail.com</li> </ul>"},{"location":"highentDCA_usage/","title":"Usage Guide","text":"<p>This guide covers how to use highentDCA for training entropy-decimated DCA models, including command-line interface usage, parameter tuning, and practical examples.</p>"},{"location":"highentDCA_usage/#quick-start","title":"Quick Start","text":"<p>The simplest way to train an edDCA model:</p> <pre><code>highentDCA train \\\n    --data your_alignment.fasta \\\n    --output results \\\n    --model edDCA \\\n    --density 0.02\n</code></pre> <p>This command trains a sparse edDCA model with 2% coupling density using default parameters.</p>"},{"location":"highentDCA_usage/#command-line-interface","title":"Command-Line Interface","text":"<p>highentDCA provides a command-line interface through the <code>highentDCA</code> command. Currently, the main command is <code>train</code>.</p>"},{"location":"highentDCA_usage/#basic-syntax","title":"Basic Syntax","text":"<pre><code>highentDCA train [OPTIONS]\n</code></pre>"},{"location":"highentDCA_usage/#required-arguments","title":"Required Arguments","text":""},{"location":"highentDCA_usage/#-data-d","title":"<code>--data</code> / <code>-d</code>","text":"<p>Path to the input multiple sequence alignment (MSA) in FASTA format.</p> <pre><code>--data example_data/PF00072.fasta\n</code></pre> <p>Requirements: - FASTA format with aligned sequences - Minimum ~1000 sequences recommended - All sequences must have the same length - Quality-controlled alignment (remove fragments, handle gaps)</p>"},{"location":"highentDCA_usage/#model-selection","title":"Model Selection","text":""},{"location":"highentDCA_usage/#-model-m","title":"<code>--model</code> / <code>-m</code>","text":"<p>Choose the type of DCA model to train. For highentDCA, use <code>edDCA</code>.</p> <pre><code>--model edDCA\n</code></pre> <p>Options: - <code>bmDCA</code>: Fully-connected Boltzmann Machine (standard DCA) - <code>eaDCA</code>: Edge-adding DCA (progressive sparsity) - <code>edDCA</code>: Entropy-decimated DCA (progressive decimation with entropy tracking)</p>"},{"location":"highentDCA_usage/#output-options","title":"Output Options","text":""},{"location":"highentDCA_usage/#-output-o","title":"<code>--output</code> / <code>-o</code>","text":"<p>Directory where model outputs will be saved. Default: <code>DCA_model</code></p> <pre><code>--output my_results\n</code></pre> <p>Output structure:</p> <pre><code>my_results/\n\u251c\u2500\u2500 params.dat                  # Final model parameters\n\u251c\u2500\u2500 chains.fasta               # Final Markov chains\n\u251c\u2500\u2500 adabmDCA_highent.log      # Training log\n\u251c\u2500\u2500 entropy_decimation/        # Checkpoints at different densities\n\u2502   \u251c\u2500\u2500 density_0.980.fasta\n\u2502   \u251c\u2500\u2500 density_0.587.fasta\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 entropy_values.txt         # Entropy vs density data\n</code></pre>"},{"location":"highentDCA_usage/#-label-l","title":"<code>--label</code> / <code>-l</code>","text":"<p>Add a custom label to output files. Optional.</p> <pre><code>--label PF00072_run1\n</code></pre> <p>Output files will be named: <code>PF00072_run1_params.dat</code>, <code>PF00072_run1_chains.fasta</code>, etc.</p>"},{"location":"highentDCA_usage/#eddca-specific-parameters","title":"edDCA-Specific Parameters","text":""},{"location":"highentDCA_usage/#decimation-parameters","title":"Decimation Parameters","text":""},{"location":"highentDCA_usage/#-density","title":"<code>--density</code>","text":"<p>Target coupling density to reach (fraction of couplings to keep). Default: <code>0.02</code> (2%)</p> <pre><code>--density 0.05  # Keep 5% of couplings\n</code></pre> <p>Typical values: - <code>0.02</code> - Very sparse (2% of couplings) - <code>0.05</code> - Sparse (5% of couplings) - <code>0.10</code> - Moderately sparse (10% of couplings)</p> <p>Guidelines: - Lower density = sparser model, faster inference - Too low may lose important information - Protein families: 2-5% usually sufficient - RNA families: 5-10% may be needed</p>"},{"location":"highentDCA_usage/#-drate","title":"<code>--drate</code>","text":"<p>Decimation rate: fraction of remaining couplings to prune at each step. Default: <code>0.01</code> (1%)</p> <pre><code>--drate 0.02  # Prune 2% of remaining couplings per step\n</code></pre> <p>Trade-offs: - Smaller <code>drate</code> (e.g., 0.005): Slower but more gradual decimation - Larger <code>drate</code> (e.g., 0.05): Faster but less refined - Recommended: 0.01-0.02 for most applications</p>"},{"location":"highentDCA_usage/#-nsweeps_dec","title":"<code>--nsweeps_dec</code>","text":"<p>Number of Monte Carlo sweeps per gradient update during decimation. Default: <code>10</code></p> <pre><code>--nsweeps_dec 20\n</code></pre> <p>Guidelines: - Increase for better equilibration (slower training) - Decrease for faster training (may reduce accuracy) - Typical range: 5-50</p>"},{"location":"highentDCA_usage/#entropy-computation-parameters","title":"Entropy Computation Parameters","text":"<p>At pre-defined density checkpoints, highentDCA computes model entropy using thermodynamic integration.</p>"},{"location":"highentDCA_usage/#-theta_max","title":"<code>--theta_max</code>","text":"<p>Maximum integration strength for thermodynamic integration. Default: <code>5.0</code></p> <pre><code>--theta_max 10.0\n</code></pre> <p>Higher values provide better integration range but require more sampling.</p>"},{"location":"highentDCA_usage/#-nsteps","title":"<code>--nsteps</code>","text":"<p>Number of integration steps for entropy computation. Default: <code>100</code></p> <pre><code>--nsteps 200\n</code></pre> <p>More steps = more accurate entropy estimate (but slower).</p>"},{"location":"highentDCA_usage/#-nsweeps_step","title":"<code>--nsweeps_step</code>","text":"<p>Number of MC sweeps per integration step. Default: <code>100</code></p> <pre><code>--nsweeps_step 50\n</code></pre>"},{"location":"highentDCA_usage/#-nsweeps_theta","title":"<code>--nsweeps_theta</code>","text":"<p>Number of sweeps to equilibrate at \u03b8_max. Default: <code>100</code></p> <pre><code>--nsweeps_theta 200\n</code></pre>"},{"location":"highentDCA_usage/#-nsweeps_zero","title":"<code>--nsweeps_zero</code>","text":"<p>Number of sweeps to equilibrate at \u03b8=0. Default: <code>100</code></p> <pre><code>--nsweeps_zero 200\n</code></pre> <p>Entropy computation tips: - Increase all nsweeps values for better accuracy - Decrease for faster (but less accurate) entropy estimates - Default values are usually sufficient</p>"},{"location":"highentDCA_usage/#general-training-parameters","title":"General Training Parameters","text":"<p>These parameters control the overall training process.</p>"},{"location":"highentDCA_usage/#convergence-criteria","title":"Convergence Criteria","text":""},{"location":"highentDCA_usage/#-target-t","title":"<code>--target</code> / <code>-t</code>","text":"<p>Target Pearson correlation between model and data statistics. Default: <code>0.95</code></p> <pre><code>--target 0.98  # Stricter convergence\n</code></pre> <p>Guidelines: - <code>0.90-0.95</code>: Standard convergence - <code>0.95-0.98</code>: High accuracy (slower training) - <code>&gt;0.98</code>: Very strict (may not converge for complex families)</p>"},{"location":"highentDCA_usage/#-nepochs","title":"<code>--nepochs</code>","text":"<p>Maximum number of training epochs. Default: <code>50000</code></p> <pre><code>--nepochs 100000\n</code></pre> <p>Training stops when either <code>--target</code> or <code>--nepochs</code> is reached.</p>"},{"location":"highentDCA_usage/#sampling-parameters","title":"Sampling Parameters","text":""},{"location":"highentDCA_usage/#-sampler","title":"<code>--sampler</code>","text":"<p>MCMC sampling method. Default: <code>gibbs</code></p> <pre><code>--sampler metropolis\n</code></pre> <p>Options: - <code>gibbs</code>: Gibbs sampling (default, usually faster) - <code>metropolis</code>: Metropolis-Hastings sampling</p>"},{"location":"highentDCA_usage/#-nsweeps","title":"<code>--nsweeps</code>","text":"<p>Number of MC sweeps per gradient update (before decimation starts). Default: <code>10</code></p> <pre><code>--nsweeps 20\n</code></pre> <p>More sweeps = better gradient estimates but slower training.</p>"},{"location":"highentDCA_usage/#-nchains","title":"<code>--nchains</code>","text":"<p>Number of parallel Markov chains for sampling. Default: <code>10000</code></p> <pre><code>--nchains 5000   # Use fewer chains (less memory)\n--nchains 20000  # Use more chains (better statistics)\n</code></pre> <p>Guidelines: - More chains = better statistics, more GPU memory - Fewer chains = faster, less memory - Typical range: 5000-20000</p>"},{"location":"highentDCA_usage/#optimization-parameters","title":"Optimization Parameters","text":""},{"location":"highentDCA_usage/#-lr","title":"<code>--lr</code>","text":"<p>Learning rate for gradient descent. Default: <code>0.01</code></p> <pre><code>--lr 0.005  # Slower, more stable\n--lr 0.02   # Faster, may be less stable\n</code></pre> <p>Guidelines: - Start with 0.01 - Decrease if training is unstable - Increase for faster convergence (if stable)</p>"},{"location":"highentDCA_usage/#regularization","title":"Regularization","text":""},{"location":"highentDCA_usage/#-pseudocount","title":"<code>--pseudocount</code>","text":"<p>Pseudocount for smoothing empirical frequencies. Default: <code>None</code> (automatic: 1/Meff)</p> <pre><code>--pseudocount 0.5\n</code></pre> <p>Acts as regularization to prevent overfitting.</p>"},{"location":"highentDCA_usage/#sequence-processing","title":"Sequence Processing","text":""},{"location":"highentDCA_usage/#alphabet","title":"Alphabet","text":""},{"location":"highentDCA_usage/#-alphabet","title":"<code>--alphabet</code>","text":"<p>Sequence alphabet/encoding. Default: <code>protein</code></p> <pre><code>--alphabet protein  # Standard 20 amino acids + gap\n--alphabet rna      # RNA: ACGU + gap\n--alphabet dna      # DNA: ACGT + gap\n--alphabet \"ACDEFG\" # Custom alphabet\n</code></pre> <p>Built-in alphabets: - <code>protein</code>: <code>ACDEFGHIKLMNPQRSTVWY-</code> - <code>rna</code>: <code>ACGU-</code> - <code>dna</code>: <code>ACGT-</code></p> <p>Custom alphabets must include all characters present in the alignment.</p>"},{"location":"highentDCA_usage/#sequence-reweighting","title":"Sequence Reweighting","text":"<p>Sequence reweighting reduces phylogenetic bias in the dataset.</p>"},{"location":"highentDCA_usage/#-weights-w","title":"<code>--weights</code> / <code>-w</code>","text":"<p>Path to pre-computed sequence weights file. Optional.</p> <pre><code>--weights sequence_weights.txt\n</code></pre> <p>File format: one weight per line, same order as sequences in FASTA.</p>"},{"location":"highentDCA_usage/#-clustering_seqid","title":"<code>--clustering_seqid</code>","text":"<p>Sequence identity threshold for automatic reweighting. Default: <code>0.8</code> (80%)</p> <pre><code>--clustering_seqid 0.9  # Cluster at 90% identity\n</code></pre> <p>Sequences with identity \u2265 threshold are clustered, and cluster members share weight.</p>"},{"location":"highentDCA_usage/#-no_reweighting","title":"<code>--no_reweighting</code>","text":"<p>Disable automatic sequence reweighting.</p> <pre><code>--no_reweighting\n</code></pre> <p>Use if your alignment is already diversity-corrected or unweighted analysis is desired.</p>"},{"location":"highentDCA_usage/#checkpoint-options","title":"Checkpoint Options","text":""},{"location":"highentDCA_usage/#checkpoint-strategy","title":"Checkpoint Strategy","text":""},{"location":"highentDCA_usage/#-checkpoints","title":"<code>--checkpoints</code>","text":"<p>Checkpoint strategy for saving model state. Default: <code>linear</code></p> <pre><code>--checkpoints linear      # Save every N epochs\n--checkpoints acceptance  # Save when acceptance rate changes\n</code></pre> <p>For edDCA, checkpoints are also triggered at pre-defined density thresholds for entropy computation.</p>"},{"location":"highentDCA_usage/#-target_acc_rate","title":"<code>--target_acc_rate</code>","text":"<p>Target acceptance rate for acceptance-based checkpoints. Default: <code>0.5</code></p> <pre><code>--target_acc_rate 0.6\n</code></pre> <p>Only used when <code>--checkpoints acceptance</code>.</p>"},{"location":"highentDCA_usage/#experiment-tracking","title":"Experiment Tracking","text":""},{"location":"highentDCA_usage/#weights-biases","title":"Weights &amp; Biases","text":""},{"location":"highentDCA_usage/#-wandb","title":"<code>--wandb</code>","text":"<p>Enable Weights &amp; Biases logging for experiment tracking.</p> <pre><code>--wandb\n</code></pre> <p>Requires W&amp;B account and login (<code>wandb login</code>).</p> <p>Logs: - Training metrics (Pearson, entropy, density) - System metrics (GPU usage, time) - Model parameters and outputs</p>"},{"location":"highentDCA_usage/#computational-settings","title":"Computational Settings","text":""},{"location":"highentDCA_usage/#device-selection","title":"Device Selection","text":""},{"location":"highentDCA_usage/#-device","title":"<code>--device</code>","text":"<p>Computation device. Default: <code>cuda</code></p> <pre><code>--device cuda  # Use GPU\n--device cpu   # Use CPU\n</code></pre> <p>GPU is strongly recommended for large datasets.</p>"},{"location":"highentDCA_usage/#data-type","title":"Data Type","text":""},{"location":"highentDCA_usage/#-dtype","title":"<code>--dtype</code>","text":"<p>Numerical precision. Default: <code>float32</code></p> <pre><code>--dtype float32  # Standard precision (faster)\n--dtype float64  # Double precision (more accurate)\n</code></pre> <p><code>float32</code> is usually sufficient and faster.</p>"},{"location":"highentDCA_usage/#advanced-options","title":"Advanced Options","text":""},{"location":"highentDCA_usage/#restoration-and-continuation","title":"Restoration and Continuation","text":""},{"location":"highentDCA_usage/#-path_params-p","title":"<code>--path_params</code> / <code>-p</code>","text":"<p>Path to existing model parameters to restore training.</p> <pre><code>--path_params previous_run/params.dat\n</code></pre>"},{"location":"highentDCA_usage/#-path_chains-c","title":"<code>--path_chains</code> / <code>-c</code>","text":"<p>Path to existing chains for restoration.</p> <pre><code>--path_chains previous_run/chains.fasta\n</code></pre> <p>Use case: Continue training from a checkpoint or use a pre-trained bmDCA as starting point.</p>"},{"location":"highentDCA_usage/#test-set-evaluation","title":"Test Set Evaluation","text":""},{"location":"highentDCA_usage/#-test","title":"<code>--test</code>","text":"<p>Path to test set MSA for evaluation during training.</p> <pre><code>--test test_sequences.fasta\n</code></pre> <p>Test log-likelihood will be computed and logged (requires additional computation).</p>"},{"location":"highentDCA_usage/#random-seed","title":"Random Seed","text":""},{"location":"highentDCA_usage/#-seed","title":"<code>--seed</code>","text":"<p>Random seed for reproducibility. Default: <code>0</code></p> <pre><code>--seed 42\n</code></pre> <p>Use different seeds for multiple independent runs.</p>"},{"location":"highentDCA_usage/#complete-example-commands","title":"Complete Example Commands","text":""},{"location":"highentDCA_usage/#example-1-basic-training","title":"Example 1: Basic Training","text":"<p>Train a sparse edDCA model with default settings:</p> <pre><code>highentDCA train \\\n    --data protein_family.fasta \\\n    --output results/protein_edDCA \\\n    --model edDCA \\\n    --density 0.02 \\\n    --seed 42\n</code></pre>"},{"location":"highentDCA_usage/#example-2-high-accuracy-training","title":"Example 2: High-Accuracy Training","text":"<p>Train with stricter convergence and more sampling:</p> <pre><code>highentDCA train \\\n    --data protein_family.fasta \\\n    --output results/high_accuracy \\\n    --model edDCA \\\n    --density 0.03 \\\n    --target 0.98 \\\n    --nchains 20000 \\\n    --nsweeps 20 \\\n    --nsweeps_dec 20 \\\n    --lr 0.005 \\\n    --seed 42\n</code></pre>"},{"location":"highentDCA_usage/#example-3-fast-exploration","title":"Example 3: Fast Exploration","text":"<p>Quick training for exploratory analysis:</p> <pre><code>highentDCA train \\\n    --data protein_family.fasta \\\n    --output results/fast_run \\\n    --model edDCA \\\n    --density 0.05 \\\n    --drate 0.02 \\\n    --nchains 5000 \\\n    --nsweeps 5 \\\n    --nsweeps_dec 5 \\\n    --target 0.90 \\\n    --nsteps 50 \\\n    --nsweeps_step 50\n</code></pre>"},{"location":"highentDCA_usage/#example-4-rna-family","title":"Example 4: RNA Family","text":"<p>Train on RNA alignment with custom parameters:</p> <pre><code>highentDCA train \\\n    --data rna_family.fasta \\\n    --output results/rna_edDCA \\\n    --model edDCA \\\n    --alphabet rna \\\n    --density 0.08 \\\n    --drate 0.01 \\\n    --clustering_seqid 0.85 \\\n    --seed 123\n</code></pre>"},{"location":"highentDCA_usage/#example-5-with-weights-biases-tracking","title":"Example 5: With Weights &amp; Biases Tracking","text":"<p>Track experiments with W&amp;B:</p> <pre><code>highentDCA train \\\n    --data protein_family.fasta \\\n    --output results/wandb_run \\\n    --model edDCA \\\n    --density 0.02 \\\n    --label experiment_001 \\\n    --wandb \\\n    --seed 42\n</code></pre>"},{"location":"highentDCA_usage/#example-6-continue-from-bmdca","title":"Example 6: Continue from bmDCA","text":"<p>Start from pre-trained bmDCA model:</p> <pre><code># First train bmDCA (or use existing)\nhighentDCA train \\\n    --data protein_family.fasta \\\n    --output bmdca_model \\\n    --model bmDCA \\\n    --target 0.95\n\n# Then decimate it\nhighentDCA train \\\n    --data protein_family.fasta \\\n    --output eddca_model \\\n    --model edDCA \\\n    --path_params bmdca_model/params.dat \\\n    --path_chains bmdca_model/chains.fasta \\\n    --density 0.02\n</code></pre>"},{"location":"highentDCA_usage/#analyzing-results","title":"Analyzing Results","text":""},{"location":"highentDCA_usage/#reading-training-logs","title":"Reading Training Logs","text":"<p>The log file (<code>adabmDCA_highent.log</code>) contains training progress:</p> <pre><code>cat results/adabmDCA_highent.log\n</code></pre> <p>Example output:</p> <pre><code>Epochs     Pearson    Entropy    Density    Time      \n0          0.950      125.456    1.000      0.000\n50         0.955      120.123    0.587      120.450\n100        0.953      115.678    0.359      250.890\n</code></pre>"},{"location":"highentDCA_usage/#extracting-entropy-values","title":"Extracting Entropy Values","text":"<p>Entropy vs. density data is saved in <code>entropy_values.txt</code>:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('results/entropy_values.txt', sep='\\t')\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(df['Density'], df['Entropy'], 'o-', linewidth=2, markersize=8)\nplt.xlabel('Coupling Density', fontsize=14)\nplt.ylabel('Model Entropy', fontsize=14)\nplt.title('Entropy Evolution During Decimation', fontsize=16)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('entropy_evolution.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"highentDCA_usage/#loading-model-parameters","title":"Loading Model Parameters","text":"<p>Load trained parameters for analysis:</p> <pre><code>from adabmDCA.io import load_params\nimport torch\n\n# Load parameters\nparams, tokens, L, q = load_params('results/params.dat')\n\n# Access fields and couplings\nfields = params['bias']  # Shape: (L, q)\ncouplings = params['coupling_matrix']  # Shape: (L, q, L, q)\n\nprint(f\"Sequence length: {L}\")\nprint(f\"Alphabet size: {q}\")\nprint(f\"Number of non-zero couplings: {(couplings != 0).sum().item()}\")\n</code></pre>"},{"location":"highentDCA_usage/#visualizing-coupling-matrix","title":"Visualizing Coupling Matrix","text":"<p>Plot the sparse coupling matrix:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom adabmDCA.io import load_params\n\n# Load parameters\nparams, tokens, L, q = load_params('results/params.dat')\ncouplings = params['coupling_matrix'].cpu().numpy()\n\n# Compute Frobenius norm for each coupling\ncoupling_strength = np.linalg.norm(couplings.reshape(L, q, L, q), axis=(1, 3))\n\n# Plot\nplt.figure(figsize=(10, 10))\nplt.imshow(coupling_strength, cmap='viridis', interpolation='nearest')\nplt.colorbar(label='Coupling Strength')\nplt.xlabel('Position j')\nplt.ylabel('Position i')\nplt.title('Sparse Coupling Matrix (edDCA)')\nplt.tight_layout()\nplt.savefig('coupling_matrix.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"highentDCA_usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"highentDCA_usage/#training-doesnt-converge","title":"Training doesn't converge","text":"<p>Solutions: - Decrease learning rate: <code>--lr 0.005</code> - Increase sweeps: <code>--nsweeps 20 --nsweeps_dec 20</code> - Relax target: <code>--target 0.93</code> - Check data quality: remove fragments, check alignment</p>"},{"location":"highentDCA_usage/#out-of-memory-errors","title":"Out of memory errors","text":"<p>Solutions: - Reduce number of chains: <code>--nchains 5000</code> - Use float32: <code>--dtype float32</code> (default) - Use smaller batch size for GPU - Monitor with: <code>nvidia-smi -l 1</code></p>"},{"location":"highentDCA_usage/#training-is-too-slow","title":"Training is too slow","text":"<p>Solutions: - Ensure GPU is being used: check <code>--device cuda</code> - Reduce accuracy requirements: <code>--target 0.93</code> - Use fewer chains: <code>--nchains 8000</code> - Increase decimation rate: <code>--drate 0.02</code> - Reduce entropy computation accuracy</p>"},{"location":"highentDCA_usage/#entropy-computation-fails","title":"Entropy computation fails","text":"<p>Solutions: - Increase equilibration: <code>--nsweeps_zero 100 --nsweeps_theta 100</code> - Reduce theta_max: <code>--theta_max 3.0</code> - Check data quality and convergence</p>"},{"location":"highentDCA_usage/#best-practices","title":"Best Practices","text":"<ol> <li>Start with defaults: Begin with default parameters and adjust based on results</li> <li>Monitor convergence: Check Pearson correlation in logs</li> <li>Use appropriate density: 2-5% for most protein families</li> <li>Save checkpoints: Use <code>--label</code> to organize multiple runs</li> <li>Validate results: Check entropy evolution makes sense</li> <li>Use test sets: Evaluate generalization with <code>--test</code></li> <li>Set seeds: Use <code>--seed</code> for reproducibility</li> <li>Track experiments: Use <code>--wandb</code> for complex parameter searches</li> </ol>"},{"location":"highentDCA_usage/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference: Use highentDCA in Python scripts</li> <li>Checkpoint Documentation: Understand checkpoint strategies</li> <li>edDCA Model Documentation: Deep dive into the algorithm</li> </ul>"},{"location":"highentDCA_usage/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Check the log file for error messages</li> <li>Review GitHub Issues</li> <li>Contact: robertonetti3@gmail.com</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":"<p><code>adabmDCA</code> is available in three language-specific implementations:</p> <ul> <li>Python \u2013 optimized for GPU execution  </li> <li>Julia \u2013 designed for multi-core CPU usage  </li> <li>C++ \u2013 lightweight and single-core CPU compatible</li> </ul> <p>Follow the instructions below based on your preferred environment.</p>"},{"location":"installation/#python-gpu-oriented","title":"\ud83d\udd37 Python (GPU-oriented)","text":""},{"location":"installation/#option-1-install-from-pypi-recommended","title":"\ud83d\udd39 Option 1: Install from PyPI (Recommended)","text":"<pre><code>pip install adabmDCA\n</code></pre> <p>Fastest way to get started. This installs the latest stable release.</p>"},{"location":"installation/#option-2-install-from-github","title":"\ud83d\udd39 Option 2: Install from GitHub","text":"<p>Clone the repository and install the package locally:</p> <pre><code>git clone https://github.com/spqb/adabmDCApy.git\ncd adabmDCApy\npip install .\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCApy</p> <p>Info</p> <p>This version of the code assumes the user to be provided with a GPU. If this is not the case, we provide a Colab notebook that can be used with GPU hardware acceleration provided by Google.</p>"},{"location":"installation/#julia-multi-core-cpu","title":"\ud83d\udfe3 Julia (Multi-core CPU)","text":"<p>Make sure you\u2019ve installed Julia. Then choose one of the following:</p>"},{"location":"installation/#option-1-automatic-setup-via-shell","title":"\ud83d\udd39 Option 1: Automatic Setup via Shell","text":"<pre><code># Download main scripts\nwget -O adabmDCA.sh https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/adabmDCA.sh\nwget -O execute.jl https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/execute.jl\nchmod +x adabmDCA.sh\n\n# Install dependencies and the package\njulia --eval 'using Pkg; Pkg.add(\"ArgParse\"); Pkg.add(PackageSpec(url=\"https://github.com/spqb/adabmDCA.jl\"))'\n</code></pre>"},{"location":"installation/#option-2-manual-setup-via-julia-repl","title":"\ud83d\udd39 Option 2: Manual Setup via Julia REPL","text":"<ol> <li>Launch Julia and run:</li> </ol> <pre><code>using Pkg\nPkg.add(url=\"https://github.com/spqb/adabmDCA.jl\")\nPkg.add(\"ArgParse\")\n</code></pre> <ol> <li>Download execution scripts:</li> </ol> <pre><code>wget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/adabmDCA.sh\nwget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/execute.jl\nchmod +x adabmDCA.sh\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCA.jl</p>"},{"location":"installation/#c-single-core-cpu","title":"\ud83d\udfe6 C++ (Single-core CPU)","text":"<p>A minimal setup with no external dependencies beyond <code>make</code>.</p>"},{"location":"installation/#installation-steps","title":"\ud83d\udd39 Installation Steps","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/spqb/adabmDCAc.git\ncd adabmDCAc/src\nmake\n</code></pre> <ol> <li>Return to the root folder and make the main script executable:</li> </ol> <pre><code>chmod +x adabmDCA.sh\n</code></pre> <ol> <li>Verify installation and available options:</li> </ol> <pre><code>./adabmDCA --help\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCAc</p> <p>Tip</p> <p>All implementations share a consistent command-line interface. You can switch between them based on your hardware and performance needs without learning new syntax.</p>"},{"location":"preprocessing/","title":"Input data and preprocessing","text":""},{"location":"preprocessing/#input-data-preprocessing","title":"Input Data &amp; Preprocessing","text":""},{"location":"preprocessing/#input-format","title":"\ud83d\udce5 Input Format","text":"<p><code>adabmDCA 2.0</code> takes as input a multiple sequence alignment (MSA) in FASTA format, typically of aligned protein or RNA/DNA sequences (see Fig. 1).</p> <p>The tool supports three built-in alphabets and also allows custom alphabets, as long as they match the MSA content.</p> Type Alphabet Symbols protein <code>-, A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y</code> RNA <code>-, A, C, G, U</code> DNA <code>-, A, C, G, T</code> <p>\ud83d\udca1 Line breaks within sequences are supported.</p> <p>Figure 1: Example of a fasta file containg the MSA. </p>"},{"location":"preprocessing/#preprocessing-steps","title":"\ud83d\udd27 Preprocessing Steps","text":"<p>The following steps are applied to every input MSA:</p> <ol> <li>Remove sequences with invalid symbols.</li> <li>Remove duplicate sequences.</li> <li>Reweight sequences to correct for phylogenetic and sampling bias (optional).</li> <li>Compute empirical statistics using a pseudocount.</li> </ol>"},{"location":"preprocessing/#sequence-reweighting","title":"\u2696\ufe0f Sequence Reweighting","text":"<p>To downweight overrepresented or phylogenetically related sequences, <code>adabmDCA</code> uses a clustering threshold (default: 80% identity). The weight for sequence \\(\\mathbf{a}^{(m)}\\) is:</p> \\[ w^{(m)} = \\frac{1}{N^{(m)}} \\] <p>where \\(N^{(m)}\\) is the number of sequences that have sequence identity with \\(\\mathbf{a}^{(m)}\\) above the clustering threshold.</p> <ul> <li>Set the threshold with <code>--clustering_seqid &lt;value&gt;</code></li> <li>Disable with <code>--no_reweighting</code></li> </ul>"},{"location":"preprocessing/#pseudocount-regularization","title":"\ud83e\uddee Pseudocount Regularization","text":"<p>A small pseudocount \\(\\alpha\\) is added to frequency estimates to prevent issues with rare or unobserved symbols:</p> <ul> <li> <p>One-site frequency:  \\(f_i(a) = (1 - \\alpha) f^{\\mathrm{data}}_i(a) + \\frac{\\alpha}{q}\\)</p> </li> <li> <p>Two-site frequency: \\(f_{ij}(a, b) = (1 - \\alpha) f^{\\mathrm{data}}_{ij}(a, b) + \\frac{\\alpha}{q^2}\\)</p> </li> </ul> <p>If not set via <code>--pseudocount</code>, the default is: $$ \\alpha = \\frac{1}{M_{\\text{eff}}}, \\quad \\text{with} \\quad M_{\\text{eff}} = \\sum_{m=1}^M w^{(m)} $$ being the effective number of sequences.</p>"},{"location":"quicklist/","title":"Quicklist","text":""},{"location":"quicklist/#list-of-the-main-routines-with-standard-arguments","title":"\u26a1 List of the main routines with standard arguments","text":"<ul> <li>\ud83e\udde0 Train a <code>bmDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Resume training of a <code>bmDCA</code> model:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83c\udf31 Train an <code>eaDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --nsweeps 5\n</code></pre> <ul> <li>\ud83d\udd04 Resume training of an eaDCA model:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\u2702\ufe0f Decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83d\udd00 Train and decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt;\n</code></pre> <ul> <li>\ud83e\uddec Generate sequences from a trained model:</li> </ul> <pre><code>adabmDCA sample -p &lt;file_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li>\ud83d\udcc9 Score a sequence set:</li> </ul> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83e\uddea Generate a single mutant library from a wild type:</li> </ul> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd17 Compute contact scores via Frobenius norm:</li> </ul> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Reintegrate DCA model from experiments:</li> </ul> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <ul> <li>\ud83e\udde0 Train/test split for homologous sequences:</li> </ul> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre>"},{"location":"script_arguments/","title":"Script Arguments","text":"<p>In this section we list all the possible command-line arguments for the main routines of <code>adabmDCA 2.0</code>.</p>"},{"location":"script_arguments/#train-a-dca-model","title":"Train a DCA model","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the dataset to be used for training the model. <code>-o, --output</code> DCA_model Path to the folder where to save the model. <code>-m, --model</code> bmDCA Type of model to be trained. Possible options are <code>bmDCA</code>, <code>eaDCA</code>, and <code>edDCA</code>. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>-p, --path_params</code> None Path to the file containing the model's parameters. Required for restoring the training. <code>-c, --path_chains</code> None Path to the FASTA file containing the model's chains. Required for restoring the training. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--lr</code> 0.05 Learning rate. <code>--nsweeps</code> 10 Number of sweeps for each gradient estimation. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--nchains</code> 10000 Number of Markov chains to run in parallel. <code>--target</code> 0.95 Pearson correlation coefficient on the two-sites statistics to be reached. <code>--nepochs</code> 50000 Maximum number of epochs allowed. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--seed</code> 0 Random seed. <code>--nthreads</code>\u00b9 1 Number of threads used in the Julia multithreaded version. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#eadca-options","title":"eaDCA options","text":"Command Default value Description <code>--gsteps</code> 10 Number of gradient updates to be performed on a given graph. <code>--factivate</code> 0.001 Fraction of inactive couplings to try to activate at each graph update."},{"location":"script_arguments/#eddca-options","title":"edDCA options","text":"Command Default value Description <code>--gsteps</code> 10 The number of gradient updates applied at each step of the graph convergence process. <code>--density</code> 0.02 Target density to be reached. <code>--drate</code> 0.01 Fraction of remaining couplings to be pruned at each decimation step."},{"location":"script_arguments/#sampling-from-a-dca-model","title":"Sampling from a DCA model","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model to sample from. <code>-d, --data</code> N/A Filename of the dataset MSA. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--ngen</code> None Number of samples to generate. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>--nmeasure</code> 10000 Number of data sequences to use for computing the mixing time. The value min(<code>nmeasure</code>, len(data)) is taken. <code>--nmix</code> 2 Number of mixing times used to generate 'ngen' sequences starting from random. <code>--max_nsweeps</code> 10000 Maximum number of sweeps allowed. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--beta</code> 1.0 Inverse temperature to be used for the sampling. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#computing-dca-energies-of-a-msa","title":"Computing DCA energies of a MSA","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#generate-a-deep-mutational-scan-dms-from-a-wild-type","title":"Generate a Deep Mutational Scan (DMS) from a wild type","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA containing the wild type. If multiple sequences are present, the first one is used. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#compute-the-frobenius-contact-matrix","title":"Compute the Frobenius contact matrix","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>-l, --label</code> None If provided, adds a label to the output files inside the output folder. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version. <p>\u00b9 Used in specific versions of the software.</p>"},{"location":"training/","title":"Training","text":""},{"location":"training/#module-training","title":"module <code>training</code>","text":""},{"location":"training/#function-update_params","title":"function <code>update_params</code>","text":"<pre><code>update_params(\n    fi: Tensor,\n    fij: Tensor,\n    pi: Tensor,\n    pij: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    lr: float\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Updates the parameters of the model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-points frequencies of the data. </li> <li><code>pi</code> (torch.Tensor):  Single-point marginals of the model. </li> <li><code>pij</code> (torch.Tensor):  Two-points marginals of the model. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the interaction graph. </li> <li><code>lr</code> (float):  Learning rate. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Updated parameters. </li> </ul> <p></p>"},{"location":"training/#function-train_graph","title":"function <code>train_graph</code>","text":"<pre><code>train_graph(\n    sampler: Callable,\n    chains: Tensor,\n    mask: Tensor,\n    fi: Tensor,\n    fij: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    lr: float,\n    max_epochs: int,\n    target_pearson: float,\n    fi_test: Tensor | None = None,\n    fij_test: Tensor | None = None,\n    checkpoint: Checkpoint | None = None,\n    check_slope: bool = False,\n    log_weights: Tensor | None = None,\n    progress_bar: bool = True\n) \u2192 Tuple[Tensor, Dict[str, Tensor], Tensor, Dict[str, List[float]]]\n</code></pre> <p>Trains the model on a given graph until the target Pearson correlation is reached or the maximum number of epochs is exceeded. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function. </li> <li><code>chains</code> (torch.Tensor):  Markov chains simulated with the model. </li> <li><code>mask</code> (torch.Tensor):  Mask encoding the sparse graph. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>nsweeps</code> (int):  Number of Gibbs steps for each gradient estimation. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>max_epochs</code> (int):  Maximum number of gradient updates to be done. </li> <li><code>target_pearson</code> (float):  Target Pearson coefficient. </li> <li><code>fi_test</code> (torch.Tensor | None, optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (torch.Tensor | None, optional):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Checkpoint | None, optional):  Checkpoint class to be used for saving the model. Defaults to None. </li> <li><code>check_slope</code> (bool, optional):  Whether to take into account the slope for the convergence criterion or not. Defaults to False. </li> <li><code>log_weights</code> (torch.Tensor, optional):  Log-weights used for the online computation of the log-likelihood. Defaults to None. </li> <li><code>progress_bar</code> (bool, optional):  Whether to display a progress bar or not. Defaults to True. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]</code>:  Updated chains and parameters, log-weights for the log-likelihood computation. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/","title":"Index","text":""},{"location":"api/#api-overview","title":"API Overview","text":""},{"location":"api/#modules","title":"Modules","text":"<ul> <li><code>checkpoint</code></li> <li><code>cobalt</code></li> <li><code>dataset</code></li> <li><code>dca</code></li> <li><code>fasta</code></li> <li><code>functional</code></li> <li><code>io</code></li> <li><code>plot</code></li> <li><code>resampling</code></li> <li><code>sampling</code></li> <li><code>statmech</code></li> <li><code>stats</code></li> <li><code>utils</code></li> </ul>"},{"location":"api/#classes","title":"Classes","text":"<ul> <li><code>checkpoint.Checkpoint</code>: Helper class to save the model's parameters and chains at regular intervals during training and to log the</li> <li><code>dataset.DatasetDCA</code>: Dataset class for handling multi-sequence alignments data.</li> </ul>"},{"location":"api/#functions","title":"Functions","text":"<ul> <li><code>cobalt.prune_redundant_sequences</code>: Prunes sequences from X such that no sequence has more than 'seqid_th' fraction of its residues identical to any other sequence in the set.</li> <li><code>cobalt.run_cobalt</code>: Runs the Cobalt algorithm to split the input MSA into training and test sets.</li> <li><code>cobalt.split_train_test</code>: Splits X into two sets, T and S, such that no sequence in S has more than</li> <li><code>dca.get_contact_map</code>: Computes the contact map from the model coupling matrix.</li> <li><code>dca.get_mf_contact_map</code>: Computes the contact map from the model coupling matrix.</li> <li><code>dca.get_seqid</code>: Returns a tensor containing the sequence identities between two sets of one-hot encoded sequences.</li> <li><code>dca.get_seqid_stats</code>: - If s2 is provided, computes the mean and the standard deviation of the mean sequence identity between two sets of one-hot encoded sequences.</li> <li><code>dca.set_zerosum_gauge</code>: Sets the zero-sum gauge on the coupling matrix.</li> <li><code>fasta.compute_weights</code>: Computes the weight to be assigned to each sequence 's' in 'data' as 1 / n_clust, where 'n_clust' is the number of sequences</li> <li><code>fasta.decode_sequence</code>: Takes a numeric sequence or list of seqences in input an returns the corresponding string encoding.</li> <li><code>fasta.encode_sequence</code>: Encodes a sequence or a list of sequences into a numeric format.</li> <li><code>fasta.get_tokens</code>: Converts the alphabet into the corresponding tokens.</li> <li><code>fasta.import_from_fasta</code>: Import sequences from a fasta file. The following operations are performed:</li> <li><code>fasta.validate_alphabet</code>: Check if the chosen alphabet is compatible with the input sequences.</li> <li><code>fasta.write_fasta</code>: Generate a fasta file with the input sequences.</li> <li><code>functional.one_hot</code>: A fast one-hot encoding function faster than the PyTorch one working with torch.int32 and returning a float Tensor.</li> <li><code>io.load_chains</code>: Loads the sequences from a fasta file and returns the one-hot encoded version.</li> <li><code>io.load_params</code>: Import the parameters of the model from a file.</li> <li><code>io.load_params_oldformat</code>: Import the parameters of the model from a file. Assumes the old DCA format.</li> <li><code>io.save_chains</code>: Saves the chains in a fasta file.</li> <li><code>io.save_params</code>: Saves the parameters of the model in a file.</li> <li><code>io.save_params_oldformat</code>: Saves the parameters of the model in a file. Assumes the old DCA format.</li> <li><code>plot.plot_PCA</code>: Makes the scatter plot of the components (pc1, pc2) of the input data and shows the histograms of the components.</li> <li><code>plot.plot_autocorrelation</code>: Plots the time-autocorrelation curve of the sequence identity and the generated and data sequence identities.</li> <li><code>plot.plot_contact_map</code>: Plots the contact map.</li> <li><code>plot.plot_pearson_sampling</code>: Plots the Pearson correlation coefficient over sampling time.</li> <li><code>plot.plot_scatter_correlations</code>: Plots the scatter plot of the data and generated Cij and Cijk values.</li> <li><code>resampling.compute_mixing_time</code>: Computes the mixing time using the t and t/2 method. The sampling will halt when the mixing time is reached or</li> <li><code>sampling.get_sampler</code>: Returns the sampling function corresponding to the chosen method.</li> <li><code>sampling.gibbs_sampling</code>: Gibbs sampling.</li> <li><code>sampling.gibbs_step_independent_sites</code>: Performs a single mutation using the Gibbs sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.gibbs_step_uniform_sites</code>: Performs a single mutation using the Gibbs sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>sampling.metropolis_sampling</code>: Metropolis sampling.</li> <li><code>sampling.metropolis_step_independent_sites</code>: Performs a single mutation using the Metropolis sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.metropolis_step_uniform_sites</code>: Performs a single mutation using the Metropolis sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>statmech.compute_energy</code>: Compute the DCA energy of the sequences in X.</li> <li><code>statmech.compute_entropy</code>: Compute the entropy of the DCA model.</li> <li><code>statmech.compute_logZ_exact</code>: Compute the log-partition function of the model.</li> <li><code>statmech.compute_log_likelihood</code>: Compute the log-likelihood of the model.</li> <li><code>statmech.enumerate_states</code>: Enumerate all possible states of a system of L sites and q states.</li> <li><code>statmech.iterate_tap</code>: Iterates the TAP equations until convergence.</li> <li><code>stats.extract_Cij_from_freq</code>: Extracts the lower triangular part of the covariance matrices of the data and chains starting from the frequencies.</li> <li><code>stats.extract_Cij_from_seqs</code>: Extracts the lower triangular part of the covariance matrices of the data and chains starting from the sequences.</li> <li><code>stats.generate_unique_triplets</code>: Generates a set of unique triplets of positions. Used to compute the 3-points statistics.</li> <li><code>stats.get_correlation_two_points</code>: Computes the Pearson coefficient and the slope between the two-point frequencies of data and chains.</li> <li><code>stats.get_covariance_matrix</code>: Computes the weighted covariance matrix of the input multi sequence alignment.</li> <li><code>stats.get_freq_single_point</code>: Computes the single point frequencies of the input MSA.</li> <li><code>stats.get_freq_three_points</code>: Computes the 3-body connected correlation statistics of the input MSAs.</li> <li><code>stats.get_freq_two_points</code>: Computes the 2-points statistics of the input MSA.</li> <li><code>utils.get_device</code>: Returns the device where to store the tensors.</li> <li><code>utils.get_dtype</code>: Returns the data type of the tensors.</li> <li><code>utils.get_mask_save</code>: Returns the mask to save the upper-triangular part of the coupling matrix.</li> <li><code>utils.init_chains</code>: Initialize the chains of the DCA model. If 'fi' is provided, the chains are sampled from the</li> <li><code>utils.init_parameters</code>: Initialize the parameters of the DCA model.</li> <li><code>utils.resample_sequences</code>: Extracts nextract sequences from data with replacement according to the weights.</li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/highentDCA_checkpoint/","title":"Checkpoint Module","text":"<p>The <code>highentDCA.checkpoint</code> module provides checkpoint strategies for saving model state during training.</p>"},{"location":"api/highentDCA_checkpoint/#overview","title":"Overview","text":"<p>Checkpoints are essential for: - Saving model progress during long training runs - Creating snapshots at specific densities during decimation - Logging training metrics - Enabling training resumption after interruptions - Tracking experiments with Weights &amp; Biases</p>"},{"location":"api/highentDCA_checkpoint/#classes","title":"Classes","text":""},{"location":"api/highentDCA_checkpoint/#checkpoint-abstract-base-class","title":"<code>Checkpoint</code> (Abstract Base Class)","text":"<p>Base class for all checkpoint strategies. Defines the interface that all checkpoint implementations must follow.</p> <pre><code>from highentDCA.checkpoint import Checkpoint\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#constructor","title":"Constructor","text":"<pre><code>Checkpoint(\n    file_paths: dict,\n    tokens: str,\n    args: dict,\n    params: Dict[str, torch.Tensor] | None = None,\n    chains: torch.Tensor | None = None,\n    use_wandb: bool = False,\n)\n</code></pre> <p>Parameters:</p> <ul> <li>file_paths (<code>dict</code>): Dictionary with file paths for saving outputs.</li> <li>Required keys: <code>\"log\"</code>, <code>\"params\"</code>, <code>\"chains\"</code></li> <li> <p>Values: <code>pathlib.Path</code> objects or strings</p> </li> <li> <p>tokens (<code>str</code>): Alphabet for sequence encoding (e.g., <code>\"protein\"</code>, <code>\"rna\"</code>, or custom)</p> </li> <li> <p>args (<code>dict</code>): Training configuration dictionary with keys:</p> </li> <li><code>\"model\"</code>: Model type (<code>\"bmDCA\"</code>, <code>\"eaDCA\"</code>, <code>\"edDCA\"</code>)</li> <li><code>\"data\"</code>: Path to input MSA</li> <li><code>\"alphabet\"</code>: Sequence alphabet</li> <li><code>\"lr\"</code>: Learning rate</li> <li><code>\"nsweeps\"</code>: Number of sweeps</li> <li><code>\"target\"</code>: Target Pearson correlation</li> <li> <p>Additional model-specific args</p> </li> <li> <p>params (<code>Dict[str, torch.Tensor] | None</code>): Initial model parameters</p> </li> <li><code>\"bias\"</code>: Fields, shape <code>(L, q)</code></li> <li> <p><code>\"coupling_matrix\"</code>: Couplings, shape <code>(L, q, L, q)</code></p> </li> <li> <p>chains (<code>torch.Tensor | None</code>): Initial Markov chains, shape <code>(n_chains, L, q)</code></p> </li> <li> <p>use_wandb (<code>bool</code>): Enable Weights &amp; Biases logging (default: <code>False</code>)</p> </li> </ul>"},{"location":"api/highentDCA_checkpoint/#methods","title":"Methods","text":""},{"location":"api/highentDCA_checkpoint/#header_log","title":"<code>header_log()</code>","text":"<p>Write the header row to the log file.</p> <pre><code>checkpoint.header_log()\n</code></pre> <p>Example:</p> <pre><code>checkpoint = DecCheckpoint(...)\ncheckpoint.header_log()\n# Writes: \"Epochs     Pearson    Entropy    Density    Time      \"\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#logrecord-dictstr-any","title":"<code>log(record: Dict[str, Any])</code>","text":"<p>Log training metrics and append to log file.</p> <pre><code>checkpoint.log(record: Dict[str, Any])\n</code></pre> <p>Parameters:</p> <ul> <li>record (<code>Dict[str, Any]</code>): Metrics to log. Valid keys:</li> <li><code>\"Epochs\"</code>: Epoch number (int)</li> <li><code>\"Pearson\"</code>: Pearson correlation (float)</li> <li><code>\"Entropy\"</code>: Model entropy (float)</li> <li><code>\"Density\"</code>: Coupling density (float)</li> <li><code>\"Time\"</code>: Elapsed time in seconds (float)</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If record contains unrecognized keys</li> </ul> <p>Example:</p> <pre><code>checkpoint.log({\n    \"Epochs\": 100,\n    \"Pearson\": 0.95,\n    \"Entropy\": 120.5,\n    \"Density\": 0.587,\n    \"Time\": 125.3,\n})\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#checkargs-kwargs-abstract","title":"<code>check(*args, **kwargs)</code> (Abstract)","text":"<p>Check if a checkpoint condition is met. Must be implemented by subclasses.</p> <pre><code>should_save = checkpoint.check(...)\n</code></pre> <p>Returns:</p> <ul> <li><code>bool</code>: <code>True</code> if checkpoint should be saved, <code>False</code> otherwise</li> </ul>"},{"location":"api/highentDCA_checkpoint/#saveparams-mask-chains-log_weights-abstract","title":"<code>save(params, mask, chains, log_weights)</code> (Abstract)","text":"<p>Save model state to disk. Must be implemented by subclasses.</p> <pre><code>checkpoint.save(\n    params=params,\n    mask=mask,\n    chains=chains,\n    log_weights=log_weights,\n)\n</code></pre> <p>Parameters:</p> <ul> <li>params (<code>Dict[str, torch.Tensor]</code>): Model parameters</li> <li>mask (<code>torch.Tensor</code>): Binary coupling mask, shape <code>(L, q, L, q)</code></li> <li>chains (<code>torch.Tensor</code>): Markov chains, shape <code>(n_chains, L, q)</code></li> <li>log_weights (<code>torch.Tensor</code>): AIS log-weights, shape <code>(n_chains,)</code></li> </ul>"},{"location":"api/highentDCA_checkpoint/#deccheckpoint-density-based-checkpoint","title":"<code>DecCheckpoint</code> (Density-Based Checkpoint)","text":"<p>Checkpoint strategy based on coupling matrix density thresholds. Saves model state when density crosses predefined values.</p> <pre><code>from highentDCA.checkpoint import DecCheckpoint\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#constructor_1","title":"Constructor","text":"<pre><code>DecCheckpoint(\n    file_paths: dict,\n    tokens: str,\n    args: dict,\n    params: Dict[str, torch.Tensor] | None = None,\n    chains: torch.Tensor | None = None,\n    checkpt_steps: list[float] | None = None,\n    use_wandb: bool = False,\n    target_density: float | None = None,\n    n_steps: int = 10,\n    **kwargs,\n)\n</code></pre> <p>Additional Parameters:</p> <ul> <li>checkpt_steps (<code>list[float] | None</code>): Custom density thresholds (e.g., <code>[0.9, 0.5, 0.2, 0.05]</code>)</li> <li>If <code>None</code>, generates <code>n_steps</code> geometrically-spaced values from <code>0.99</code> to <code>target_density</code></li> <li> <p>Values are automatically sorted in descending order</p> </li> <li> <p>target_density (<code>float | None</code>): Final target density (used to generate checkpt_steps if None provided)</p> </li> <li> <p>n_steps (<code>int</code>): Number of checkpoint steps to generate (default: 10)</p> </li> </ul> <p>Automatic Checkpoint Generation:</p> <p>If <code>checkpt_steps=None</code>, generates geometric sequence:</p> \\[\\rho_i = \\rho_{\\text{start}} \\cdot \\left(\\frac{\\rho_{\\text{target}}}{\\rho_{\\text{start}}}\\right)^{\\frac{i}{N-1}}\\] <p>where \\(\\rho_{\\text{start}} = 0.99\\), \\(\\rho_{\\text{target}}\\) = <code>target_density</code>, and \\(N\\) = <code>n_steps</code>.</p> <p>Example:</p> <pre><code>from pathlib import Path\nfrom highentDCA.checkpoint import DecCheckpoint\n\ncheckpoint = DecCheckpoint(\n    file_paths={\n        \"log\": Path(\"output/training.log\"),\n        \"params\": Path(\"output/params.dat\"),\n        \"chains\": Path(\"output/chains.fasta\"),\n    },\n    tokens=\"protein\",\n    args={\n        \"model\": \"edDCA\",\n        \"data\": \"data/alignment.fasta\",\n        \"alphabet\": \"protein\",\n        \"lr\": 0.01,\n        \"nsweeps\": 10,\n        \"nsweeps_dec\": 10,\n        \"target\": 0.95,\n        \"density\": 0.02,\n        \"drate\": 0.01,\n        \"pseudocount\": None,\n        \"dtype\": \"float32\",\n        \"label\": None,\n        \"nepochs\": 50000,\n        \"sampler\": \"gibbs\",\n        \"nchains\": 10000,\n        \"seed\": 42,\n    },\n    target_density=0.02,\n    n_steps=10,\n    use_wandb=False,\n)\n\n# Automatic checkpoints at:\n# [0.99, 0.82, 0.68, 0.56, 0.46, 0.38, 0.31, 0.26, 0.21, 0.02]\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#methods_1","title":"Methods","text":""},{"location":"api/highentDCA_checkpoint/#checkdensity-float","title":"<code>check(density: float)</code>","text":"<p>Check if current density crossed a checkpoint threshold.</p> <pre><code>should_save = checkpoint.check(density=0.35)\n</code></pre> <p>Parameters:</p> <ul> <li>density (<code>float</code>): Current coupling matrix density</li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>: <code>True</code> if density crossed a threshold, <code>False</code> otherwise</li> </ul> <p>Behavior:</p> <ul> <li>Automatically removes all passed thresholds</li> <li>Allows skipping intermediate checkpoints if density drops rapidly</li> <li>Returns <code>False</code> when all checkpoints exhausted</li> </ul> <p>Example:</p> <pre><code>checkpoint = DecCheckpoint(\n    ...,\n    checkpt_steps=[0.9, 0.5, 0.2, 0.05],\n)\n\ncheckpoint.check(0.85)  # False (0.85 &gt; 0.9)\ncheckpoint.check(0.60)  # True (crossed 0.9)\ncheckpoint.check(0.45)  # True (crossed 0.5)\ncheckpoint.check(0.18)  # True (crossed 0.2, skipped)\ncheckpoint.check(0.03)  # True (crossed 0.05)\ncheckpoint.check(0.02)  # False (all exhausted)\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#saveparams-mask-chains-log_weights-density-float","title":"<code>save(params, mask, chains, log_weights, density: float)</code>","text":"<p>Save model state with density-labeled filenames.</p> <pre><code>checkpoint.save(\n    params=params,\n    mask=mask,\n    chains=chains,\n    log_weights=log_weights,\n    density=0.35,\n)\n</code></pre> <p>Parameters:</p> <ul> <li>density (<code>float</code>): Current density (appended to filenames)</li> <li>Other parameters: Same as base <code>Checkpoint.save()</code></li> </ul> <p>Output Files:</p> <p>Creates two files with density labels:</p> <ul> <li><code>{stem}_density_{density:.3f}.dat</code> - Parameters file</li> <li><code>{stem}_density_{density:.3f}.fasta</code> - Chains file</li> </ul> <p>Example:</p> <pre><code>checkpoint.save(\n    params=params,\n    mask=mask,\n    chains=chains,\n    log_weights=log_weights,\n    density=0.587,\n)\n\n# Creates:\n# - output/params_density_0.587.dat\n# - output/chains_density_0.587.fasta\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#complete-usage-example","title":"Complete Usage Example","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom adabmDCA.dataset import DatasetDCA\nfrom adabmDCA.utils import init_chains, init_parameters\nfrom adabmDCA.sampling import get_sampler\nfrom highentDCA.models.edDCA import fit\nfrom highentDCA.checkpoint import DecCheckpoint\n\n# Configuration\ndevice = torch.device(\"cuda\")\ndtype = torch.float32\n\n# Load dataset\ndataset = DatasetDCA(\n    path_data=\"data/PF00072.fasta\",\n    alphabet=\"protein\",\n    device=device,\n    dtype=dtype,\n)\n\n# Initialize model\nparams = init_parameters(L=dataset.L, q=dataset.q, device=device, dtype=dtype)\nchains = init_chains(10000, dataset.L, dataset.q, device=device, dtype=dtype)\nlog_weights = torch.zeros(10000, device=device, dtype=dtype)\nsampler = get_sampler(\"gibbs\")\n\n# Create output directory\noutput_dir = Path(\"results/PF00072\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Configure checkpoint with custom densities\ncheckpoint = DecCheckpoint(\n    file_paths={\n        \"log\": output_dir / \"training.log\",\n        \"params\": output_dir / \"params.dat\",\n        \"chains\": output_dir / \"chains.fasta\",\n    },\n    tokens=dataset.tokens,\n    args={\n        \"model\": \"edDCA\",\n        \"data\": \"data/PF00072.fasta\",\n        \"alphabet\": \"protein\",\n        \"lr\": 0.01,\n        \"nsweeps\": 10,\n        \"nsweeps_dec\": 10,\n        \"target\": 0.95,\n        \"density\": 0.02,\n        \"drate\": 0.01,\n        \"pseudocount\": None,\n        \"dtype\": \"float32\",\n        \"label\": \"PF00072_edDCA\",\n        \"nepochs\": 50000,\n        \"sampler\": \"gibbs\",\n        \"nchains\": 10000,\n        \"seed\": 42,\n    },\n    checkpt_steps=[0.98, 0.80, 0.60, 0.40, 0.20, 0.10, 0.05, 0.02],\n    target_density=0.02,\n    use_wandb=False,\n)\n\n# Initialize log header\ncheckpoint.header_log()\n\n# Train model with checkpoint\nmask = torch.ones(\n    dataset.L, dataset.q, dataset.L, dataset.q,\n    device=device, dtype=torch.bool\n)\n\nfit(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    fi_target=dataset.fi,\n    fij_target=dataset.fij,\n    params=params,\n    mask=mask,\n    lr=0.01,\n    nsweeps=10,\n    target_pearson=0.95,\n    target_density=0.02,\n    drate=0.01,\n    checkpoint=checkpoint,\n    args=checkpoint.args,\n)\n\nprint(\"Training complete!\")\nprint(f\"Results saved in: {output_dir}\")\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Enable experiment tracking with W&amp;B:</p> <pre><code>checkpoint = DecCheckpoint(\n    ...,\n    use_wandb=True,  # Enable W&amp;B\n)\n\n# Metrics automatically logged to W&amp;B:\n# - Epochs\n# - Pearson correlation\n# - Entropy\n# - Coupling density\n# - Training time\n</code></pre> <p>Prerequisites:</p> <pre><code>pip install wandb\nwandb login\n</code></pre>"},{"location":"api/highentDCA_checkpoint/#see-also","title":"See Also","text":"<ul> <li>Training Module - Training functions using checkpoints</li> <li>edDCA Model - Model fitting with checkpoints</li> <li>Usage Guide - CLI checkpoint options</li> </ul>"},{"location":"api/highentDCA_entropy/","title":"Entropy Computation Module","text":"<p>The <code>highentDCA.scripts.entropy</code> module implements thermodynamic integration for computing model entropy.</p>"},{"location":"api/highentDCA_entropy/#overview","title":"Overview","text":"<p>Model entropy is computed by integrating the average sequence identity with respect to a bias parameter \u03b8 that guides sampling towards a target sequence. This thermodynamic integration approach provides accurate entropy estimates for Boltzmann machines.</p>"},{"location":"api/highentDCA_entropy/#functions","title":"Functions","text":""},{"location":"api/highentDCA_entropy/#compute_entropy","title":"<code>compute_entropy()</code>","text":"<p>Compute the entropy of a DCA model using thermodynamic integration.</p> <pre><code>from highentDCA.scripts.entropy import compute_entropy\n\nentropy = compute_entropy(\n    params: Dict[str, torch.Tensor],\n    path_targetseq: str,\n    sampler: Callable,\n    chains: torch.Tensor,\n    output: str,\n    label: str,\n    tokens: str,\n    nsweeps_zero: int = 100,\n    nsweeps_theta: int = 100,\n    theta_max: float = 5.0,\n    nsteps: int = 100,\n    nsweeps: int = 100,\n    device: str = \"cuda\",\n    dtype: str = \"float32\",\n) -&gt; float\n</code></pre>"},{"location":"api/highentDCA_entropy/#parameters","title":"Parameters","text":"<ul> <li>params (<code>Dict[str, torch.Tensor]</code>): Model parameters</li> <li><code>\"bias\"</code>: Fields, shape <code>(L, q)</code></li> <li> <p><code>\"coupling_matrix\"</code>: Couplings, shape <code>(L, q, L, q)</code></p> </li> <li> <p>path_targetseq (<code>str</code>): Path to FASTA file with target sequence</p> </li> <li>Must contain at least one sequence</li> <li>If multiple sequences, first one is used</li> <li> <p>Typically the wild-type or reference sequence</p> </li> <li> <p>sampler (<code>Callable</code>): Sampling function</p> </li> <li> <p>Signature: <code>sampler(chains, params, nsweeps) -&gt; chains</code></p> </li> <li> <p>chains (<code>torch.Tensor</code>): Initial Markov chains, shape <code>(n_chains, L, q)</code></p> </li> <li> <p>output (<code>str</code>): Path to output directory for log files</p> </li> <li> <p>label (<code>str</code>): Label for output files (e.g., density value)</p> </li> <li> <p>Creates: <code>density_{label}.log</code></p> </li> <li> <p>tokens (<code>str</code>): Sequence alphabet (e.g., <code>\"protein\"</code>, <code>\"rna\"</code>)</p> </li> <li> <p>nsweeps_zero (<code>int</code>): Sweeps for equilibration at \u03b8=0 (default: 100)</p> </li> <li> <p>nsweeps_theta (<code>int</code>): Sweeps for equilibration at \u03b8=\u03b8_max (default: 100)</p> </li> <li> <p>theta_max (<code>float</code>): Initial maximum bias strength (default: 5.0)</p> </li> <li> <p>Automatically adjusted to achieve ~10% target sequences</p> </li> <li> <p>nsteps (<code>int</code>): Number of integration steps (default: 100)</p> </li> <li> <p>More steps = more accurate (but slower)</p> </li> <li> <p>nsweeps (<code>int</code>): Sweeps per integration step (default: 100)</p> </li> <li> <p>device (<code>str</code>): Computation device (<code>\"cuda\"</code> or <code>\"cpu\"</code>)</p> </li> <li> <p>dtype (<code>str</code>): Data type (<code>\"float32\"</code> or <code>\"float64\"</code>)</p> </li> </ul>"},{"location":"api/highentDCA_entropy/#returns","title":"Returns","text":"<ul> <li><code>float</code>: Computed entropy value</li> </ul>"},{"location":"api/highentDCA_entropy/#raises","title":"Raises","text":"<ul> <li><code>FileNotFoundError</code>: If <code>path_targetseq</code> doesn't exist</li> <li><code>ValueError</code>: If <code>label</code> is <code>None</code></li> </ul>"},{"location":"api/highentDCA_entropy/#algorithm","title":"Algorithm","text":"<p>The thermodynamic integration method computes entropy as:</p> \\[S = E_0 - F(\\theta_{\\max})\\] <p>where: - \\(E_0\\) = average energy at \u03b8=0 (unbiased model) - \\(F(\\theta_{\\max})\\) = free energy at \u03b8=\u03b8_max</p>"},{"location":"api/highentDCA_entropy/#step-1-equilibrate-at-0","title":"Step 1: Equilibrate at \u03b8=0","text":"<p>Sample from unbiased model:</p> <pre><code>params_0 = params.copy()\nchains_0 = sampler(chains, params_0, nsweeps_zero)\nE_0 = mean(energy(chains_0, params_0))\n</code></pre>"},{"location":"api/highentDCA_entropy/#step-2-equilibrate-at-_max","title":"Step 2: Equilibrate at \u03b8=\u03b8_max","text":"<p>Sample with bias towards target sequence:</p> <pre><code>params_theta = params.copy()\nparams_theta[\"bias\"] += theta_max * target_seq  # Add bias\n\nchains_theta = sampler(chains_theta, params_theta, nsweeps_theta)\n</code></pre>"},{"location":"api/highentDCA_entropy/#step-3-adjust-_max","title":"Step 3: Adjust \u03b8_max","text":"<p>Ensure ~10% of chains match target sequence:</p> <pre><code>while p_target &lt; 0.1:\n    theta_max *= 1.01\n    params_theta[\"bias\"] = params[\"bias\"] + theta_max * target_seq\n    chains_theta = sampler(chains_theta, params_theta, 100)\n    p_target = fraction_matching_target(chains_theta)\n</code></pre>"},{"location":"api/highentDCA_entropy/#step-4-thermodynamic-integration","title":"Step 4: Thermodynamic Integration","text":"<p>Integrate average sequence identity from \u03b8=0 to \u03b8_max:</p> <pre><code>F = log(p_target) + mean(energy(chains_at_target, params_theta))\n\nfor theta in linspace(0, theta_max, nsteps):\n    params_theta[\"bias\"] = params[\"bias\"] + theta * target_seq\n    chains = sampler(chains, params_theta, nsweeps)\n\n    seqID = sequence_identity(chains, target_seq)\n    F += integration_weight * mean(seqID)  # Trapezoidal rule\n\nS = E_0 - F\n</code></pre>"},{"location":"api/highentDCA_entropy/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom adabmDCA.io import load_params\nfrom adabmDCA.utils import init_chains\nfrom adabmDCA.sampling import get_sampler\nfrom highentDCA.scripts.entropy import compute_entropy\n\n# Load trained model\nparams, tokens, L, q = load_params(\"results/params.dat\")\n\n# Initialize chains\ndevice = torch.device(\"cuda\")\ndtype = torch.float32\nchains = init_chains(\n    nchains=10000,\n    L=L,\n    q=q,\n    device=device,\n    dtype=dtype,\n)\n\n# Get sampler\nsampler = get_sampler(\"gibbs\")\n\n# Create output directory\noutput_dir = Path(\"results/entropy_analysis\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Compute entropy\nprint(\"Computing model entropy...\")\nS = compute_entropy(\n    params=params,\n    path_targetseq=\"data/wildtype.fasta\",\n    sampler=sampler,\n    chains=chains,\n    output=str(output_dir),\n    label=\"final_model\",\n    tokens=tokens,\n    nsweeps_zero=200,\n    nsweeps_theta=200,\n    theta_max=5.0,\n    nsteps=200,\n    nsweeps=100,\n    device=\"cuda\",\n    dtype=\"float32\",\n)\n\nprint(f\"Model entropy: {S:.4f}\")\nprint(f\"Log saved: {output_dir}/density_final_model.log\")\n</code></pre>"},{"location":"api/highentDCA_entropy/#output-files","title":"Output Files","text":"<p>The function creates a detailed log file: <code>density_{label}.log</code></p>"},{"location":"api/highentDCA_entropy/#log-file-format","title":"Log File Format","text":"<pre><code>density:              final_model                                       \nnchains:              10000                                             \nnsweeps:              100                                               \nnsweeps  theta:       200                                               \nnsweeps zero:         200                                               \nnsteps:               200                                               \ndata type:            float32                                           \n\nEpoch           Theta           Free Energy     Entropy         Time           \n0               0.000           50.123          125.456         0.000          \n1               0.025           50.145          125.434         1.234          \n2               0.050           50.178          125.401         2.456          \n...\n199             4.975           30.234          145.345         245.678        \n</code></pre>"},{"location":"api/highentDCA_entropy/#analyzing-entropy-results","title":"Analyzing Entropy Results","text":""},{"location":"api/highentDCA_entropy/#reading-log-files","title":"Reading Log Files","text":"<pre><code>import pandas as pd\n\n# Parse log file (skip header lines)\ndf = pd.read_csv(\n    \"results/entropy_analysis/density_final_model.log\",\n    sep=r'\\s+',\n    skiprows=8,  # Skip configuration lines\n    names=[\"Epoch\", \"Theta\", \"Free_Energy\", \"Entropy\", \"Time\"],\n)\n\nprint(f\"Final entropy: {df['Entropy'].iloc[-1]:.4f}\")\nprint(f\"Computation time: {df['Time'].iloc[-1]:.1f} seconds\")\n</code></pre>"},{"location":"api/highentDCA_entropy/#plotting-integration-progress","title":"Plotting Integration Progress","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Free energy evolution\naxes[0].plot(df['Theta'], df['Free_Energy'], linewidth=2)\naxes[0].set_xlabel('\u03b8')\naxes[0].set_ylabel('Free Energy F(\u03b8)')\naxes[0].set_title('Free Energy Integration')\naxes[0].grid(True, alpha=0.3)\n\n# Entropy evolution\naxes[1].plot(df['Theta'], df['Entropy'], linewidth=2)\naxes[1].set_xlabel('\u03b8')\naxes[1].set_ylabel('Entropy S(\u03b8)')\naxes[1].set_title('Entropy Evolution')\naxes[1].grid(True, alpha=0.3)\n\n# Convergence check\naxes[2].plot(df['Epoch'], df['Entropy'], linewidth=2)\naxes[2].axhline(y=df['Entropy'].iloc[-1], color='r', linestyle='--', alpha=0.5)\naxes[2].set_xlabel('Integration Step')\naxes[2].set_ylabel('Entropy')\naxes[2].set_title('Integration Convergence')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('entropy_integration.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"api/highentDCA_entropy/#integration-with-eddca","title":"Integration with edDCA","text":"<p>During edDCA training, entropy is computed at density checkpoints:</p> <pre><code>from highentDCA.models.edDCA import fit\n\n# fit() automatically calls compute_entropy at checkpoints\nfit(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    fi_target=dataset.fi,\n    fij_target=dataset.fij,\n    params=params,\n    mask=mask,\n    lr=0.01,\n    nsweeps=10,\n    target_pearson=0.95,\n    target_density=0.02,\n    drate=0.01,\n    checkpoint=checkpoint,\n    args=args,  # Must contain entropy parameters\n)\n\n# Results saved in:\n# - output/entropy_decimation/density_0.980.log\n# - output/entropy_decimation/density_0.587.log\n# - etc.\n</code></pre>"},{"location":"api/highentDCA_entropy/#performance-tuning","title":"Performance Tuning","text":""},{"location":"api/highentDCA_entropy/#fast-less-accurate","title":"Fast (Less Accurate)","text":"<pre><code>S = compute_entropy(\n    ...,\n    nsweeps_zero=50,\n    nsweeps_theta=50,\n    nsteps=50,\n    nsweeps=50,\n)\n# ~10x faster, \u00b15% accuracy\n</code></pre>"},{"location":"api/highentDCA_entropy/#standard-default","title":"Standard (Default)","text":"<pre><code>S = compute_entropy(\n    ...,\n    nsweeps_zero=100,\n    nsweeps_theta=100,\n    nsteps=100,\n    nsweeps=100,\n)\n# Balanced speed/accuracy, \u00b12% accuracy\n</code></pre>"},{"location":"api/highentDCA_entropy/#high-accuracy","title":"High Accuracy","text":"<pre><code>S = compute_entropy(\n    ...,\n    nsweeps_zero=200,\n    nsweeps_theta=200,\n    nsteps=200,\n    nsweeps=200,\n)\n# ~4x slower, \u00b11% accuracy\n</code></pre>"},{"location":"api/highentDCA_entropy/#theoretical-background","title":"Theoretical Background","text":""},{"location":"api/highentDCA_entropy/#thermodynamic-integration","title":"Thermodynamic Integration","text":"<p>The entropy of a Boltzmann machine is related to its partition function:</p> \\[S = \\langle E \\rangle - F\\] <p>where \\(F = -\\log Z\\) is the free energy.</p> <p>By introducing a bias parameter \u03b8:</p> \\[H_\\theta(\\mathbf{s}) = H(\\mathbf{s}) - \\theta \\sum_i s_i \\cdot s_i^{\\text{target}}\\] <p>We can compute:</p> \\[F(\\theta) = F(0) + \\int_0^\\theta \\left\\langle \\sum_i s_i \\cdot s_i^{\\text{target}} \\right\\rangle_{\\theta'} d\\theta'\\]"},{"location":"api/highentDCA_entropy/#sequence-identity","title":"Sequence Identity","text":"<p>The integrand is the average sequence identity:</p> \\[\\text{seqID}(\\mathbf{s}) = \\frac{1}{L} \\sum_{i=1}^L \\delta_{s_i, s_i^{\\text{target}}}\\]"},{"location":"api/highentDCA_entropy/#numerical-integration","title":"Numerical Integration","text":"<p>Trapezoidal rule:</p> \\[\\int_0^{\\theta_{\\max}} f(\\theta) d\\theta \\approx \\frac{\\Delta\\theta}{2} \\left[ f(0) + 2\\sum_{i=1}^{N-1} f(i\\Delta\\theta) + f(\\theta_{\\max}) \\right]\\] <p>where \\(\\Delta\\theta = \\theta_{\\max} / N\\).</p>"},{"location":"api/highentDCA_entropy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/highentDCA_entropy/#low-target-sequence-percentage-5","title":"Low target sequence percentage (&lt;5%)","text":"<p>Solution: Increase <code>theta_max</code>:</p> <pre><code>S = compute_entropy(..., theta_max=10.0)\n</code></pre>"},{"location":"api/highentDCA_entropy/#entropy-oscillates","title":"Entropy oscillates","text":"<p>Solution: Increase equilibration:</p> <pre><code>S = compute_entropy(\n    ...,\n    nsweeps_zero=200,\n    nsweeps_theta=200,\n    nsweeps=200,\n)\n</code></pre>"},{"location":"api/highentDCA_entropy/#out-of-memory","title":"Out of memory","text":"<p>Solution: Reduce number of chains:</p> <pre><code>chains = init_chains(5000, L, q)  # Instead of 10000\n</code></pre>"},{"location":"api/highentDCA_entropy/#see-also","title":"See Also","text":"<ul> <li>edDCA Model - Automatic entropy computation</li> <li>Statistical Mechanics - Energy and partition function</li> <li>Barrat-Charlaix et al., 2021 - Entropy decimation paper</li> </ul>"},{"location":"api/highentDCA_models.edDCA/","title":"edDCA Model Module","text":"<p>The <code>highentDCA.models.edDCA</code> module implements the entropy-decimated Direct Coupling Analysis (edDCA) algorithm.</p>"},{"location":"api/highentDCA_models.edDCA/#overview","title":"Overview","text":"<p>The edDCA algorithm progressively decimates (sparsifies) a fully-connected DCA model while: - Maintaining model accuracy (high Pearson correlation with data) - Computing entropy at key density checkpoints via thermodynamic integration - Tracking the relationship between model complexity and information content</p>"},{"location":"api/highentDCA_models.edDCA/#functions","title":"Functions","text":""},{"location":"api/highentDCA_models.edDCA/#fit","title":"<code>fit()</code>","text":"<p>Main function for training an entropy-decimated DCA model.</p> <pre><code>from highentDCA.models.edDCA import fit\n\nfit(\n    sampler: Callable,\n    chains: torch.Tensor,\n    log_weights: torch.Tensor,\n    fi_target: torch.Tensor,\n    fij_target: torch.Tensor,\n    params: Dict[str, torch.Tensor],\n    mask: torch.Tensor,\n    lr: float,\n    nsweeps: int,\n    target_pearson: float,\n    target_density: float,\n    drate: float,\n    checkpoint: Checkpoint,\n    fi_test: torch.Tensor | None = None,\n    fij_test: torch.Tensor | None = None,\n    args=None,\n    *extra_args,\n    **kwargs,\n) -&gt; None\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#parameters","title":"Parameters","text":"<ul> <li>sampler (<code>Callable</code>): Sampling function for updating Markov chains</li> <li>Signature: <code>sampler(chains, params, nsweeps) -&gt; chains</code></li> <li> <p>Get via: <code>from adabmDCA.sampling import get_sampler</code></p> </li> <li> <p>chains (<code>torch.Tensor</code>): Initial Markov chains</p> </li> <li>Shape: <code>(n_chains, L, q)</code> in one-hot encoding</li> <li> <p>Initialize with: <code>from adabmDCA.utils import init_chains</code></p> </li> <li> <p>log_weights (<code>torch.Tensor</code>): Log-weights for Annealed Importance Sampling</p> </li> <li>Shape: <code>(n_chains,)</code></li> <li> <p>Initialize: <code>torch.zeros(n_chains, device=device, dtype=dtype)</code></p> </li> <li> <p>fi_target (<code>torch.Tensor</code>): Single-point frequencies from training data</p> </li> <li>Shape: <code>(L, q)</code></li> <li> <p>Compute with: <code>from adabmDCA.stats import get_freq_single_point</code></p> </li> <li> <p>fij_target (<code>torch.Tensor</code>): Two-point frequencies from training data</p> </li> <li>Shape: <code>(L, q, L, q)</code></li> <li> <p>Compute with: <code>from adabmDCA.stats import get_freq_two_points</code></p> </li> <li> <p>params (<code>Dict[str, torch.Tensor]</code>): Model parameters</p> </li> <li>Keys: <code>\"bias\"</code>, <code>\"coupling_matrix\"</code></li> <li><code>\"bias\"</code>: Fields, shape <code>(L, q)</code></li> <li><code>\"coupling_matrix\"</code>: Couplings, shape <code>(L, q, L, q)</code></li> <li> <p>Initialize with: <code>from adabmDCA.utils import init_parameters</code></p> </li> <li> <p>mask (<code>torch.Tensor</code>): Binary mask for coupling matrix</p> </li> <li>Shape: <code>(L, q, L, q)</code></li> <li>Initial: <code>torch.ones_like(params[\"coupling_matrix\"])</code></li> <li> <p>Updated during decimation</p> </li> <li> <p>lr (<code>float</code>): Learning rate for gradient descent</p> </li> <li>Typical values: 0.005 - 0.02</li> <li> <p>Default: 0.01</p> </li> <li> <p>nsweeps (<code>int</code>): Monte Carlo sweeps per gradient update (initial training)</p> </li> <li>Typical values: 5 - 50</li> <li> <p>Default: 10</p> </li> <li> <p>target_pearson (<code>float</code>): Target Pearson correlation for convergence</p> </li> <li>Range: 0.90 - 0.99</li> <li>Default: 0.95</li> <li> <p>Higher = stricter convergence</p> </li> <li> <p>target_density (<code>float</code>): Target coupling density to reach</p> </li> <li>Range: 0.01 - 0.20</li> <li>Default: 0.02 (2% of couplings)</li> <li> <p>Lower = sparser model</p> </li> <li> <p>drate (<code>float</code>): Decimation rate (fraction of couplings to prune per step)</p> </li> <li>Range: 0.005 - 0.05</li> <li>Default: 0.01 (1%)</li> <li> <p>Smaller = more gradual decimation</p> </li> <li> <p>checkpoint (<code>Checkpoint</code>): Checkpoint object for saving progress</p> </li> <li>Type: <code>DecCheckpoint</code> (recommended for edDCA)</li> <li> <p>See: Checkpoint Module</p> </li> <li> <p>fi_test (<code>torch.Tensor | None</code>): Single-point frequencies from test data</p> </li> <li>Shape: <code>(L, q)</code></li> <li> <p>Optional: For test set evaluation</p> </li> <li> <p>fij_test (<code>torch.Tensor | None</code>): Two-point frequencies from test data</p> </li> <li>Shape: <code>(L, q, L, q)</code></li> <li> <p>Optional: For test set evaluation</p> </li> <li> <p>args: Training arguments (namespace or dict)</p> </li> <li>Must contain entropy computation parameters:<ul> <li><code>theta_max</code>, <code>nsteps</code>, <code>nsweeps_step</code></li> <li><code>nsweeps_zero</code>, <code>nsweeps_theta</code></li> <li><code>data</code> (path to target sequence for entropy)</li> </ul> </li> </ul>"},{"location":"api/highentDCA_models.edDCA/#returns","title":"Returns","text":"<ul> <li><code>None</code>: Function modifies <code>params</code>, <code>chains</code>, <code>mask</code> in-place and saves via checkpoint</li> </ul>"},{"location":"api/highentDCA_models.edDCA/#raises","title":"Raises","text":"<ul> <li><code>ValueError</code>: If input tensors have incorrect dimensions</li> <li><code>fi_target</code> must be 2D</li> <li><code>fij_target</code> must be 4D</li> <li><code>chains</code> must be 3D</li> </ul>"},{"location":"api/highentDCA_models.edDCA/#algorithm","title":"Algorithm","text":"<p>The edDCA algorithm consists of the following steps:</p>"},{"location":"api/highentDCA_models.edDCA/#1-initial-convergence","title":"1. Initial Convergence","text":"<p>If the model isn't already converged (Pearson &lt; target):</p> <pre><code>chains, params, log_weights, _ = train_graph(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    mask=mask,\n    fi=fi_target,\n    fij=fij_target,\n    params=params,\n    nsweeps=nsweeps,\n    lr=lr,\n    max_epochs=MAX_EPOCHS,\n    target_pearson=target_pearson,\n    check_slope=False,\n    checkpoint=checkpoint,\n    progress_bar=False,\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#2-compute-initial-entropy","title":"2. Compute Initial Entropy","text":"<p>Before decimation starts:</p> <pre><code>S_initial = compute_entropy(\n    params=params,\n    path_targetseq=args.data,\n    sampler=sampler,\n    chains=chains,\n    output=output_folder,\n    label=f\"{density:.3f}\",\n    tokens=checkpoint.tokens,\n    ...\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#3-decimation-loop","title":"3. Decimation Loop","text":"<p>While <code>density &gt; target_density</code>:</p>"},{"location":"api/highentDCA_models.edDCA/#a-decimate-graph","title":"a. Decimate Graph","text":"<pre><code>from adabmDCA.graph import decimate_graph\n\nparams, mask = decimate_graph(\n    pij=pij,\n    params=params,\n    mask=mask,\n    drate=drate,\n)\n</code></pre> <p>Removes <code>drate</code> fraction of weakest couplings based on empirical two-point statistics.</p>"},{"location":"api/highentDCA_models.edDCA/#b-update-ais-weights","title":"b. Update AIS Weights","text":"<pre><code>from adabmDCA.statmech import _update_weights_AIS\n\nlog_weights = _update_weights_AIS(\n    prev_params=prev_params,\n    curr_params=params,\n    chains=chains,\n    log_weights=log_weights,\n)\n</code></pre> <p>Tracks partition function changes for log-likelihood estimation.</p>"},{"location":"api/highentDCA_models.edDCA/#c-equilibrate-chains","title":"c. Equilibrate Chains","text":"<pre><code>chains = sampler(\n    chains=chains,\n    params=params,\n    nsweeps=args.nsweeps_dec,\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#d-re-converge-model","title":"d. Re-converge Model","text":"<pre><code>chains, params, log_weights, _ = train_graph(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    mask=mask,\n    fi=fi_target,\n    fij=fij_target,\n    params=params,\n    nsweeps=args.nsweeps_dec,\n    lr=lr,\n    max_epochs=MAX_EPOCHS,\n    target_pearson=target_pearson_dec,\n    check_slope=False,\n    progress_bar=False,\n    checkpoint=None,\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#e-checkpoint-entropy","title":"e. Checkpoint &amp; Entropy","text":"<p>If density crossed a checkpoint threshold:</p> <pre><code>if checkpoint_dec.check(density):\n    S = compute_entropy(...)\n\n    checkpoint_dec.log({\n        \"Epochs\": count,\n        \"Pearson\": pearson,\n        \"Entropy\": S,\n        \"Density\": density,\n        \"Time\": elapsed_time,\n    })\n\n    checkpoint_dec.save(\n        params=params,\n        mask=mask,\n        chains=chains,\n        log_weights=log_weights,\n        density=density,\n    )\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#4-final-save","title":"4. Final Save","text":"<p>After reaching target density:</p> <pre><code>checkpoint_dec.save(\n    params=params,\n    mask=mask,\n    chains=chains,\n    log_weights=log_weights,\n    density=density,\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#complete-example","title":"Complete Example","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom adabmDCA.dataset import DatasetDCA\nfrom adabmDCA.utils import init_chains, init_parameters, get_device, get_dtype\nfrom adabmDCA.sampling import get_sampler\nfrom highentDCA.models.edDCA import fit\nfrom highentDCA.checkpoint import DecCheckpoint\n\n# Configuration\ndevice = get_device(\"cuda\")\ndtype = get_dtype(\"float32\")\n\n# Load dataset\ndataset = DatasetDCA(\n    path_data=\"data/PF00072.fasta\",\n    alphabet=\"protein\",\n    clustering_seqid=0.8,\n    device=device,\n    dtype=dtype,\n)\n\nprint(f\"Dataset: {dataset.M} sequences, L={dataset.L}, q={dataset.q}\")\n\n# Initialize parameters and chains\nparams = init_parameters(L=dataset.L, q=dataset.q, device=device, dtype=dtype)\nchains = init_chains(\n    nchains=10000,\n    L=dataset.L,\n    q=dataset.q,\n    device=device,\n    dtype=dtype,\n)\nlog_weights = torch.zeros(chains.shape[0], device=device, dtype=dtype)\n\n# Configure sampler\nsampler = get_sampler(\"gibbs\")\n\n# Set up output\noutput_dir = Path(\"results/PF00072_edDCA\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Create checkpoint\ncheckpoint = DecCheckpoint(\n    file_paths={\n        \"log\": output_dir / \"training.log\",\n        \"params\": output_dir / \"params.dat\",\n        \"chains\": output_dir / \"chains.fasta\",\n    },\n    tokens=dataset.tokens,\n    args={\n        \"model\": \"edDCA\",\n        \"data\": \"data/PF00072.fasta\",\n        \"alphabet\": \"protein\",\n        \"lr\": 0.01,\n        \"nsweeps\": 10,\n        \"nsweeps_dec\": 10,\n        \"target\": 0.95,\n        \"density\": 0.02,\n        \"drate\": 0.01,\n        \"pseudocount\": None,\n        \"dtype\": \"float32\",\n        \"label\": None,\n        \"nepochs\": 50000,\n        \"sampler\": \"gibbs\",\n        \"nchains\": 10000,\n        \"seed\": 42,\n        # Entropy parameters\n        \"theta_max\": 5.0,\n        \"nsteps\": 100,\n        \"nsweeps_step\": 100,\n        \"nsweeps_theta\": 100,\n        \"nsweeps_zero\": 100,\n    },\n    target_density=0.02,\n    n_steps=10,\n)\n\n# Create args namespace for entropy computation\nclass Args:\n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\nargs = Args(\n    data=\"data/PF00072.fasta\",\n    nsweeps_zero=100,\n    nsweeps_theta=100,\n    theta_max=5.0,\n    nsteps=100,\n    nsweeps_step=100,\n    nsweeps_dec=10,\n)\n\n# Initialize full coupling mask\nmask = torch.ones(\n    dataset.L, dataset.q, dataset.L, dataset.q,\n    device=device,\n    dtype=torch.bool,\n)\n\n# Train edDCA model\nprint(\"\\nStarting edDCA training...\")\nfit(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    fi_target=dataset.fi,\n    fij_target=dataset.fij,\n    params=params,\n    mask=mask,\n    lr=0.01,\n    nsweeps=10,\n    target_pearson=0.95,\n    target_density=0.02,\n    drate=0.01,\n    checkpoint=checkpoint,\n    args=args,\n)\n\nprint(f\"\\nTraining complete! Results in: {output_dir}\")\nprint(f\"- Parameters: {output_dir}/params.dat\")\nprint(f\"- Chains: {output_dir}/chains.fasta\")\nprint(f\"- Log: {output_dir}/training.log\")\nprint(f\"- Entropy data: {output_dir}/entropy_decimation/entropy_values.txt\")\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#analyzing-results","title":"Analyzing Results","text":""},{"location":"api/highentDCA_models.edDCA/#load-saved-models","title":"Load Saved Models","text":"<pre><code>from adabmDCA.io import load_params, load_chains\n\n# Load final model\nparams, tokens, L, q = load_params(\"results/PF00072_edDCA/params.dat\")\n\n# Load checkpoint at specific density\nparams_587, _, _, _ = load_params(\n    \"results/PF00072_edDCA/entropy_decimation/params_density_0.587.dat\"\n)\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#visualize-entropy-evolution","title":"Visualize Entropy Evolution","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load entropy data\nentropy_df = pd.read_csv(\n    \"results/PF00072_edDCA/entropy_decimation/entropy_values.txt\",\n    sep='\\t',\n)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(entropy_df['Density'], entropy_df['Entropy'], 'o-', linewidth=2)\nplt.xlabel('Coupling Density', fontsize=14)\nplt.ylabel('Model Entropy', fontsize=14)\nplt.title('Entropy Evolution During Decimation', fontsize=16)\nplt.grid(True, alpha=0.3)\nplt.xscale('log')\nplt.tight_layout()\nplt.savefig('entropy_evolution.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#analyze-coupling-sparsity","title":"Analyze Coupling Sparsity","text":"<pre><code>import torch\nimport numpy as np\n\n# Load parameters\nparams, tokens, L, q = load_params(\"results/PF00072_edDCA/params.dat\")\n\n# Count non-zero couplings\ncouplings = params['coupling_matrix']\ntotal_couplings = L * (L - 1) / 2 * q * q  # Upper triangle only\nnon_zero = (couplings.abs() &gt; 1e-10).sum().item() / 2  # Upper triangle\n\ndensity = non_zero / total_couplings\nprint(f\"Final density: {density:.4f}\")\nprint(f\"Non-zero couplings: {int(non_zero):,} / {int(total_couplings):,}\")\n\n# Compute sparsity per position pair\ncoupling_strength = torch.linalg.norm(\n    couplings.reshape(L, q, L, q),\n    dim=(1, 3),\n).cpu().numpy()\n\n# Plot coupling matrix\nplt.figure(figsize=(10, 10))\nplt.imshow(coupling_strength, cmap='viridis')\nplt.colorbar(label='Coupling Strength (Frobenius norm)')\nplt.xlabel('Position j')\nplt.ylabel('Position i')\nplt.title(f'Sparse Coupling Matrix (density={density:.3f})')\nplt.tight_layout()\nplt.savefig('coupling_matrix.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"api/highentDCA_models.edDCA/#performance-tips","title":"Performance Tips","text":"<ol> <li>GPU Memory: Reduce <code>nchains</code> if out of memory</li> <li>Speed: Increase <code>drate</code> for faster (but less refined) decimation</li> <li>Accuracy: Increase <code>nsweeps_dec</code> for better equilibration</li> <li>Entropy: Reduce <code>nsteps</code> for faster (less accurate) entropy estimates</li> </ol>"},{"location":"api/highentDCA_models.edDCA/#see-also","title":"See Also","text":"<ul> <li>Training Module - Graph training function</li> <li>Checkpoint Module - Checkpoint strategies</li> <li>Entropy Module - Thermodynamic integration</li> <li>Usage Guide - CLI usage</li> </ul>"},{"location":"api/highentDCA_overview/","title":"API Reference","text":"<p>Welcome to the highentDCA Python API documentation. This section provides detailed information about the modules, classes, and functions available in the highentDCA package.</p>"},{"location":"api/highentDCA_overview/#overview","title":"Overview","text":"<p>highentDCA extends the <code>adabmDCA</code> framework with specialized functionality for entropy-decimated DCA models. The package is organized into several modules:</p> <ul> <li>Checkpoint: Checkpoint strategies for saving model state</li> <li>Training: Training functions for graph-based DCA models</li> <li>edDCA Model: Entropy decimation algorithm implementation</li> <li>CLI: Command-line interface entry point</li> <li>Parser: Argument parsing utilities</li> <li>Entropy Computation: Thermodynamic integration for entropy calculation</li> </ul>"},{"location":"api/highentDCA_overview/#quick-links","title":"Quick Links","text":""},{"location":"api/highentDCA_overview/#core-modules","title":"Core Modules","text":"Module Description <code>highentDCA.models.edDCA</code> Entropy decimation fitting algorithm <code>highentDCA.training</code> Graph training utilities <code>highentDCA.checkpoint</code> Checkpoint management classes <code>highentDCA.parser</code> CLI argument parsers <code>highentDCA.scripts.entropy</code> Entropy computation via thermodynamic integration"},{"location":"api/highentDCA_overview/#common-imports","title":"Common Imports","text":"<pre><code># Model training\nfrom highentDCA.models.edDCA import fit\n\n# Training utilities\nfrom highentDCA.training import train_graph\n\n# Checkpoint classes\nfrom highentDCA.checkpoint import Checkpoint, DecCheckpoint\n\n# Argument parsing\nfrom highentDCA.parser import add_args_train, add_args_edDCA\n\n# Entropy computation\nfrom highentDCA.scripts.entropy import compute_entropy\n</code></pre>"},{"location":"api/highentDCA_overview/#usage-examples","title":"Usage Examples","text":""},{"location":"api/highentDCA_overview/#example-1-basic-eddca-training","title":"Example 1: Basic edDCA Training","text":"<pre><code>import torch\nfrom pathlib import Path\nfrom adabmDCA.dataset import DatasetDCA\nfrom adabmDCA.utils import init_chains, init_parameters, get_device\nfrom adabmDCA.sampling import get_sampler\nfrom highentDCA.models.edDCA import fit\nfrom highentDCA.checkpoint import DecCheckpoint\n\n# Configuration\ndevice = get_device(\"cuda\")\ndtype = torch.float32\n\n# Load dataset\ndataset = DatasetDCA(\n    path_data=\"data/protein_family.fasta\",\n    alphabet=\"protein\",\n    device=device,\n    dtype=dtype,\n)\n\n# Initialize parameters and chains\nparams = init_parameters(L=dataset.L, q=dataset.q, device=device, dtype=dtype)\nchains = init_chains(\n    nchains=10000,\n    L=dataset.L,\n    q=dataset.q,\n    device=device,\n    dtype=dtype,\n)\nlog_weights = torch.zeros(chains.shape[0], device=device, dtype=dtype)\n\n# Set up sampler\nsampler = get_sampler(\"gibbs\")\n\n# Configure checkpoint\ncheckpoint = DecCheckpoint(\n    file_paths={\n        \"log\": Path(\"output/training.log\"),\n        \"params\": Path(\"output/params.dat\"),\n        \"chains\": Path(\"output/chains.fasta\"),\n    },\n    tokens=dataset.tokens,\n    args={\n        \"model\": \"edDCA\",\n        \"data\": \"data/protein_family.fasta\",\n        \"alphabet\": \"protein\",\n        \"density\": 0.02,\n        \"drate\": 0.01,\n        # ... other args\n    },\n    target_density=0.02,\n)\n\n# Train edDCA model\nfit(\n    sampler=sampler,\n    chains=chains,\n    log_weights=log_weights,\n    fi_target=dataset.fi,\n    fij_target=dataset.fij,\n    params=params,\n    mask=torch.ones_like(params[\"coupling_matrix\"]),\n    lr=0.01,\n    nsweeps=10,\n    target_pearson=0.95,\n    target_density=0.02,\n    drate=0.01,\n    checkpoint=checkpoint,\n)\n</code></pre>"},{"location":"api/highentDCA_overview/#example-2-custom-checkpoint-strategy","title":"Example 2: Custom Checkpoint Strategy","text":"<pre><code>from highentDCA.checkpoint import DecCheckpoint\n\n# Create custom density checkpoints\ncustom_densities = [0.9, 0.7, 0.5, 0.3, 0.1, 0.05, 0.02]\n\ncheckpoint = DecCheckpoint(\n    file_paths={\n        \"log\": Path(\"output/custom.log\"),\n        \"params\": Path(\"output/params.dat\"),\n        \"chains\": Path(\"output/chains.fasta\"),\n    },\n    tokens=\"protein\",\n    args=training_args,\n    checkpt_steps=custom_densities,\n    target_density=0.02,\n)\n</code></pre>"},{"location":"api/highentDCA_overview/#example-3-computing-entropy","title":"Example 3: Computing Entropy","text":"<pre><code>from highentDCA.scripts.entropy import compute_entropy\nfrom adabmDCA.io import load_params\nfrom adabmDCA.sampling import get_sampler\n\n# Load trained model\nparams, tokens, L, q = load_params(\"output/params.dat\")\n\n# Initialize chains\nchains = init_chains(nchains=10000, L=L, q=q, device=\"cuda\")\n\n# Get sampler\nsampler = get_sampler(\"gibbs\")\n\n# Compute entropy\nentropy = compute_entropy(\n    params=params,\n    path_targetseq=\"data/target_sequence.fasta\",\n    sampler=sampler,\n    chains=chains,\n    output=\"output/entropy\",\n    label=\"density_0.020\",\n    tokens=tokens,\n    theta_max=5.0,\n    nsteps=100,\n    nsweeps=100,\n    device=\"cuda\",\n)\n\nprint(f\"Model entropy: {entropy:.4f}\")\n</code></pre>"},{"location":"api/highentDCA_overview/#example-4-training-on-specific-graph","title":"Example 4: Training on Specific Graph","text":"<pre><code>from highentDCA.training import train_graph\nimport torch\n\n# Create sparse mask (e.g., contact map)\nmask = torch.zeros(L, q, L, q, device=device, dtype=torch.bool)\n# ... populate mask with desired interactions ...\n\n# Train on this specific graph\nchains, params, log_weights, history = train_graph(\n    sampler=sampler,\n    chains=chains,\n    mask=mask,\n    fi=dataset.fi,\n    fij=dataset.fij,\n    params=params,\n    nsweeps=10,\n    lr=0.01,\n    max_epochs=10000,\n    target_pearson=0.95,\n    checkpoint=checkpoint,\n)\n\n# Access training history\nimport matplotlib.pyplot as plt\nplt.plot(history[\"epochs\"], history[\"pearson\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Pearson Correlation\")\nplt.show()\n</code></pre>"},{"location":"api/highentDCA_overview/#integration-with-adabmdca","title":"Integration with adabmDCA","text":"<p>highentDCA is built on top of <code>adabmDCA</code>, so you have access to all adabmDCA functionality:</p> <pre><code># Import adabmDCA utilities\nfrom adabmDCA.fasta import import_from_fasta, write_fasta\nfrom adabmDCA.stats import get_freq_single_point, get_freq_two_points\nfrom adabmDCA.io import load_params, save_params\nfrom adabmDCA.sampling import gibbs_sampling, metropolis\nfrom adabmDCA.statmech import compute_energy, compute_log_likelihood\nfrom adabmDCA.graph import decimate_graph, compute_density\n\n# Use with highentDCA\nfrom highentDCA.models.edDCA import fit\nfrom highentDCA.checkpoint import DecCheckpoint\n</code></pre>"},{"location":"api/highentDCA_overview/#type-hints","title":"Type Hints","text":"<p>highentDCA uses Python type hints for better code documentation and IDE support:</p> <pre><code>from typing import Dict, Callable\nimport torch\n\ndef fit(\n    sampler: Callable,\n    chains: torch.Tensor,\n    log_weights: torch.Tensor,\n    fi_target: torch.Tensor,\n    fij_target: torch.Tensor,\n    params: Dict[str, torch.Tensor],\n    mask: torch.Tensor,\n    lr: float,\n    nsweeps: int,\n    target_pearson: float,\n    target_density: float,\n    drate: float,\n    checkpoint: Checkpoint,\n    fi_test: torch.Tensor | None = None,\n    fij_test: torch.Tensor | None = None,\n    args=None,\n) -&gt; None:\n    ...\n</code></pre>"},{"location":"api/highentDCA_overview/#module-details","title":"Module Details","text":"<p>Click on the links below for detailed documentation of each module:</p> <ul> <li>Checkpoint: Learn about checkpoint strategies</li> <li>Training: Understand graph training functions</li> <li>edDCA Model: Deep dive into entropy decimation</li> <li>CLI: Command-line interface implementation</li> <li>Parser: Argument parsing utilities</li> <li>Entropy Computation: Thermodynamic integration details</li> </ul>"},{"location":"api/highentDCA_overview/#contributing","title":"Contributing","text":"<p>To contribute to the API:</p> <ol> <li>Follow PEP 8 style guidelines</li> <li>Add type hints to all function signatures</li> <li>Write comprehensive docstrings (Google style)</li> <li>Include examples in docstrings where appropriate</li> <li>Update this documentation when adding new features</li> </ol>"},{"location":"api/highentDCA_overview/#see-also","title":"See Also","text":"<ul> <li>adabmDCA API Documentation</li> <li>PyTorch Documentation</li> <li>Usage Guide</li> </ul>"},{"location":"api/highentDCA_training/","title":"Training Module","text":"<p>The <code>highentDCA.training</code> module provides functions for training DCA models on sparse graphs.</p>"},{"location":"api/highentDCA_training/#functions","title":"Functions","text":""},{"location":"api/highentDCA_training/#train_graph","title":"<code>train_graph()</code>","text":"<p>Trains a DCA model on a fixed sparse graph using gradient descent until convergence.</p> <pre><code>from highentDCA.training import train_graph\n\nchains, params, log_weights, history = train_graph(\n    sampler: Callable,\n    chains: torch.Tensor,\n    mask: torch.Tensor,\n    fi: torch.Tensor,\n    fij: torch.Tensor,\n    params: Dict[str, torch.Tensor],\n    nsweeps: int,\n    lr: float,\n    max_epochs: int,\n    target_pearson: float,\n    fi_test: torch.Tensor | None = None,\n    fij_test: torch.Tensor | None = None,\n    checkpoint: Checkpoint | None = None,\n    check_slope: bool = False,\n    log_weights: torch.Tensor | None = None,\n    progress_bar: bool = True,\n) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]\n</code></pre>"},{"location":"api/highentDCA_training/#parameters","title":"Parameters","text":"<ul> <li>sampler (<code>Callable</code>): Sampling function</li> <li> <p>Signature: <code>sampler(chains, params, nsweeps) -&gt; chains</code></p> </li> <li> <p>chains (<code>torch.Tensor</code>): Markov chains, shape <code>(n_chains, L, q)</code></p> </li> <li> <p>mask (<code>torch.Tensor</code>): Binary coupling mask, shape <code>(L, q, L, q)</code></p> </li> <li><code>mask[i,a,j,b] = 1</code>: coupling (i,a)-(j,b) is active</li> <li> <p><code>mask[i,a,j,b] = 0</code>: coupling is fixed to zero</p> </li> <li> <p>fi (<code>torch.Tensor</code>): Single-point target frequencies, shape <code>(L, q)</code></p> </li> <li> <p>fij (<code>torch.Tensor</code>): Two-point target frequencies, shape <code>(L, q, L, q)</code></p> </li> <li> <p>params (<code>Dict[str, torch.Tensor]</code>): Model parameters</p> </li> <li><code>\"bias\"</code>: shape <code>(L, q)</code></li> <li> <p><code>\"coupling_matrix\"</code>: shape <code>(L, q, L, q)</code></p> </li> <li> <p>nsweeps (<code>int</code>): MC sweeps per gradient update</p> </li> <li> <p>lr (<code>float</code>): Learning rate</p> </li> <li> <p>max_epochs (<code>int</code>): Maximum training epochs</p> </li> <li> <p>target_pearson (<code>float</code>): Target Pearson correlation (0-1)</p> </li> <li> <p>fi_test (<code>torch.Tensor | None</code>): Test set single-point frequencies (optional)</p> </li> <li> <p>fij_test (<code>torch.Tensor | None</code>): Test set two-point frequencies (optional)</p> </li> <li> <p>checkpoint (<code>Checkpoint | None</code>): Checkpoint object (optional)</p> </li> <li> <p>check_slope (<code>bool</code>): Also check correlation slope \u2248 1.0 (default: <code>False</code>)</p> </li> <li> <p>log_weights (<code>torch.Tensor | None</code>): AIS log-weights, shape <code>(n_chains,)</code> (optional)</p> </li> <li> <p>progress_bar (<code>bool</code>): Show tqdm progress bar (default: <code>True</code>)</p> </li> </ul>"},{"location":"api/highentDCA_training/#returns","title":"Returns","text":"<p><code>Tuple</code> containing:</p> <ol> <li>chains (<code>torch.Tensor</code>): Updated chains, shape <code>(n_chains, L, q)</code></li> <li>params (<code>Dict[str, torch.Tensor]</code>): Trained parameters</li> <li>log_weights (<code>torch.Tensor</code>): Updated AIS weights, shape <code>(n_chains,)</code></li> <li>history (<code>Dict[str, List[float]]</code>): Training history with keys:</li> <li><code>\"epochs\"</code>: List of epoch numbers</li> <li><code>\"pearson\"</code>: Pearson correlation at each epoch</li> <li><code>\"slope\"</code>: Correlation slope at each epoch</li> </ol>"},{"location":"api/highentDCA_training/#algorithm","title":"Algorithm","text":"<p>The training process consists of:</p>"},{"location":"api/highentDCA_training/#1-parameter-update","title":"1. Parameter Update","text":"<p>At each epoch, update parameters using gradient descent:</p> <pre><code>from adabmDCA.training import update_params\n\nparams = update_params(\n    fi=fi,\n    fij=fij,\n    pi=pi,  # Model marginals from chains\n    pij=pij,  # Model two-point marginals\n    params=params,\n    mask=mask,\n    lr=lr,\n)\n</code></pre> <p>Gradient: \\(\\Delta h_i^a = \\eta (f_i^a - p_i^a)\\), \\(\\Delta J_{ij}^{ab} = \\eta (f_{ij}^{ab} - p_{ij}^{ab})\\)</p> <p>where \\(\\eta\\) is the learning rate, \\(f\\) are data frequencies, \\(p\\) are model marginals.</p>"},{"location":"api/highentDCA_training/#2-chain-sampling","title":"2. Chain Sampling","text":"<p>Update chains using MCMC:</p> <pre><code>chains = sampler(chains=chains, params=params, nsweeps=nsweeps)\n</code></pre>"},{"location":"api/highentDCA_training/#3-convergence-check","title":"3. Convergence Check","text":"<p>Check if target Pearson correlation reached:</p> <pre><code>from adabmDCA.stats import get_correlation_two_points\n\npearson, slope = get_correlation_two_points(\n    fij=fij, pij=pij, fi=fi, pi=pi\n)\n\nconverged = (pearson &gt;= target_pearson)\nif check_slope:\n    converged &amp;= (abs(slope - 1.0) &lt; 0.1)\n</code></pre>"},{"location":"api/highentDCA_training/#4-checkpointing","title":"4. Checkpointing","text":"<p>Periodically save model state:</p> <pre><code>if checkpoint is not None and checkpoint.check(epochs, params, chains):\n    checkpoint.log({...})\n    checkpoint.save(params, mask, chains, log_weights)\n</code></pre>"},{"location":"api/highentDCA_training/#example-training-on-full-graph","title":"Example: Training on Full Graph","text":"<pre><code>import torch\nfrom adabmDCA.dataset import DatasetDCA\nfrom adabmDCA.utils import init_chains, init_parameters\nfrom adabmDCA.sampling import get_sampler\nfrom highentDCA.training import train_graph\n\n# Load data\ndataset = DatasetDCA(\"data/alignment.fasta\", alphabet=\"protein\")\n\n# Initialize\nparams = init_parameters(dataset.L, dataset.q)\nchains = init_chains(10000, dataset.L, dataset.q)\nlog_weights = torch.zeros(10000)\nsampler = get_sampler(\"gibbs\")\n\n# Full coupling mask\nmask = torch.ones(dataset.L, dataset.q, dataset.L, dataset.q, dtype=torch.bool)\n\n# Train\nchains, params, log_weights, history = train_graph(\n    sampler=sampler,\n    chains=chains,\n    mask=mask,\n    fi=dataset.fi,\n    fij=dataset.fij,\n    params=params,\n    nsweeps=10,\n    lr=0.01,\n    max_epochs=10000,\n    target_pearson=0.95,\n)\n\nprint(f\"Converged after {len(history['epochs'])} epochs\")\nprint(f\"Final Pearson: {history['pearson'][-1]:.4f}\")\n</code></pre>"},{"location":"api/highentDCA_training/#example-training-on-sparse-graph","title":"Example: Training on Sparse Graph","text":"<pre><code>import torch\nfrom highentDCA.training import train_graph\n\n# Create sparse mask (e.g., from contact map or previous decimation)\nmask = torch.zeros(L, q, L, q, dtype=torch.bool)\n\n# Add specific interactions\nfor (i, j) in contact_pairs:\n    mask[i, :, j, :] = True\n    mask[j, :, i, :] = True  # Symmetric\n\nprint(f\"Mask density: {mask.float().mean():.4f}\")\n\n# Train on this sparse graph\nchains, params, log_weights, history = train_graph(\n    sampler=sampler,\n    chains=chains,\n    mask=mask,\n    fi=dataset.fi,\n    fij=dataset.fij,\n    params=params,\n    nsweeps=10,\n    lr=0.01,\n    max_epochs=10000,\n    target_pearson=0.95,\n    log_weights=log_weights,\n)\n</code></pre>"},{"location":"api/highentDCA_training/#example-with-test-set-evaluation","title":"Example: With Test Set Evaluation","text":"<pre><code>from adabmDCA.dataset import DatasetDCA\nfrom highentDCA.training import train_graph\n\n# Load train and test data\ntrain_data = DatasetDCA(\"data/train.fasta\", alphabet=\"protein\")\ntest_data = DatasetDCA(\"data/test.fasta\", alphabet=\"protein\")\n\n# Train with test set monitoring\nchains, params, log_weights, history = train_graph(\n    sampler=sampler,\n    chains=chains,\n    mask=mask,\n    fi=train_data.fi,\n    fij=train_data.fij,\n    fi_test=test_data.fi,\n    fij_test=test_data.fij,\n    params=params,\n    nsweeps=10,\n    lr=0.01,\n    max_epochs=10000,\n    target_pearson=0.95,\n)\n</code></pre>"},{"location":"api/highentDCA_training/#example-plotting-training-history","title":"Example: Plotting Training History","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Plot Pearson evolution\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nax[0].plot(history['epochs'], history['pearson'], linewidth=2)\nax[0].axhline(y=0.95, color='r', linestyle='--', label='Target')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Pearson Correlation')\nax[0].set_title('Convergence')\nax[0].legend()\nax[0].grid(True, alpha=0.3)\n\nax[1].plot(history['epochs'], history['slope'], linewidth=2)\nax[1].axhline(y=1.0, color='r', linestyle='--', label='Ideal')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Correlation Slope')\nax[1].set_title('Slope Evolution')\nax[1].legend()\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_history.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"api/highentDCA_training/#convergence-criteria","title":"Convergence Criteria","text":"<p>The <code>halt_condition</code> function determines when to stop training:</p> <pre><code>def halt_condition(epochs, pearson, slope, check_slope):\n    c1 = pearson &lt; target_pearson\n    c2 = epochs &lt; max_epochs\n    if check_slope:\n        c3 = abs(slope - 1.0) &gt; 0.1\n    else:\n        c3 = False\n    return not c2 * ((not c1) * c3 + c1)\n</code></pre> <p>Training stops when: - Pearson \u2265 <code>target_pearson</code> (AND slope \u2248 1.0 if <code>check_slope=True</code>) - OR <code>epochs</code> \u2265 <code>max_epochs</code></p>"},{"location":"api/highentDCA_training/#performance-optimization","title":"Performance Optimization","text":""},{"location":"api/highentDCA_training/#gpu-memory","title":"GPU Memory","text":"<p>Reduce memory usage:</p> <pre><code># Fewer chains\nchains = init_chains(5000, L, q)  # Instead of 10000\n\n# Lower precision\ndtype = torch.float32  # Instead of float64\n</code></pre>"},{"location":"api/highentDCA_training/#speed","title":"Speed","text":"<p>Faster training:</p> <pre><code># Fewer sweeps (if convergence is stable)\nnsweeps = 5\n\n# Higher learning rate (if stable)\nlr = 0.02\n\n# Disable progress bar\nprogress_bar = False\n</code></pre>"},{"location":"api/highentDCA_training/#accuracy","title":"Accuracy","text":"<p>Better convergence:</p> <pre><code># More sweeps\nnsweeps = 20\n\n# Lower learning rate\nlr = 0.005\n\n# Stricter target\ntarget_pearson = 0.98\ncheck_slope = True\n</code></pre>"},{"location":"api/highentDCA_training/#see-also","title":"See Also","text":"<ul> <li>edDCA Model - Uses <code>train_graph</code> for decimation</li> <li>Checkpoint Module - Checkpoint strategies</li> <li>adabmDCA Training - Base training functions</li> </ul>"}]}